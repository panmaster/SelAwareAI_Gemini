## Summary of Files and Directories in 'C:\Users\DELL\Desktop\openAIF_frontend\OctopusAI\PROJECT\SMART_BOT4'

File: focus.json (C:\Users\DELL\Desktop\openAIF_frontend\OctopusAI\PROJECT\SMART_BOT4\focus.json)
Content (First 25 lines):
{
  "focus": [
    {
      "category": "Research",
      "text": "....",
      "frustration_level": 2,
      "focus_strength": 8,
      "defocus_threshold": 5
    },
    {
      "category": "Task",
      "text": "....",
      "frustration_level": 1,
      "focus_strength": 7,
      "defocus_threshold": 4
    },
    {
      "category": "Goal",
      "text": "...",
      "frustration_level": 0,
      "focus_strength": 9,
      "defocus_threshold": 3
    }
  ]
}

File: GeminiModelRunner_Perceptron.py (C:\Users\DELL\Desktop\openAIF_frontend\OctopusAI\PROJECT\SMART_BOT4\GeminiModelRunner_Perceptron.py)
Content (First 458 lines):
import time
import os
import json
import re
import google.generativeai as genai
from typing import Dict, List, Any, Tuple
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.common.exceptions import TimeoutException

# Assuming these modules exist and are correctly implemented:
from TOOL_MANAGER import ToolManager
from tools.ai.update_focus import update_focus

# Load Google Gemini API key
from keys import googleKey as API_KEY

class Geuron_Gemini:
    """
    A class to manage interactions with the Google Gemini model,
    including web scraping, prompt construction, tool execution,
    and response processing.
    """
    def __init__(self):
        self.tools_folder = "tools"
        self.tool_manager = ToolManager(self.tools_folder)
        genai.configure(api_key=API_KEY)

    class Color:
        """ANSI color codes for enhanced console output."""
        HEADER = '\033[95m'
        OKBLUE = '\033[94m'
        OKCYAN = '\033[96m'
        OKGREEN = '\033[92m'
        WARNING = '\033[93m'
        FAIL = '\033[91m'
        ENDC = '\033[0m'  # Resets color
        BOLD = '\033[1m'
        UNDERLINE = '\033[4m'
        PURPLE = '\033[95m'
        MAGENTA = '\033[35m'
        YELLOW = '\033[33m'
        CYAN = '\033[36m'
        RED = '\033[31m'

    def print_colored(self, color, text):
        """Prints text with the specified ANSI color."""
        print(color + text + self.Color.ENDC)

    def scrape_website(self, url: str, extract_links: bool = True,
                       extract_images: bool = True,
                       extract_text: bool = True) -> Dict[str, Any]:
        """
        Scrapes data from a website using Selenium.

        Args:
            url (str): Website URL to scrape.
            extract_links (bool): Extract links (default True).
            extract_images (bool): Extract image URLs (default True).
            extract_text (bool): Extract text content (default True).

        Returns:
            Dict[str, Any]: Scraped data ('links', 'images', 'text').
        """
        print(f"Scraping website: {url}")
        scraped_data = {}

        options = webdriver.ChromeOptions()
        # options.add_argument('--headless=new') # Uncomment for headless mode
        driver = webdriver.Chrome(options=options)
        driver.get(url)

        try:
            WebDriverWait(driver, 10).until(
                EC.presence_of_element_located((By.TAG_NAME, "body"))
            )

            if extract_links:
                scraped_data['links'] = [link.get_attribute("href")
                                          for link in driver.find_elements(By.TAG_NAME, "a")]

            if extract_images:
                scraped_data['images'] = [img.get_attribute("src")
                                          for img in driver.find_elements(By.TAG_NAME, "img")]

            if extract_text:
                scraped_data['text'] = driver.find_element(By.TAG_NAME, "body").text

        except TimeoutException:
            self.print_colored(self.Color.RED,
                              f"Timeout: Page loading took too long for {url}")
        except Exception as e:
            self.print_colored(self.Color.RED, f"Web scraping error: {e}")

        driver.quit()
        return scraped_data

    def run_model(self,
                  model_name: str,
                  initial_system_instruction: str = "You are a helpful AI assistant.",
                  use_stop_loop_flags: bool = False,
                  enable_user_input: bool = False,
                  user_input_interval: int = 15,
                  max_loops: int = 10,
                  looping: bool = True,
                  data_to_include: List[str] = ["text"],
                  injection_prompts: List[str] = None,
                  input_data: Dict[str, Any] = None,
                  expected_output_type: str = None,
                  use_data_loading_flags: bool = False) -> Any:
        """
        Runs the Google Gemini model, manages the interaction loop,
        processes responses, and handles tool executions.

        Args:
            model_name (str): Name of the Gemini model to use.
            initial_system_instruction (str): Initial instructions for the model.
            use_stop_loop_flags (bool): Allow loop control with flags (default False).
            enable_user_input (bool): Enable user input during the loop (default False).
            user_input_interval (int): How often to prompt for user input (default 15).
            max_loops (int): Maximum number of interaction loops (default 10).
            looping (bool): Run in a loop (default True).
            data_to_include (List[str]): Initial data types to include ('text', 'images', 'links').
            injection_prompts (List[str]): Additional prompts to inject.
            input_data (Dict[str, Any]): Input data for the model ('urls', etc.).
            expected_output_type (str): Expected output type ('json', 'text', etc.).
            use_data_loading_flags (bool): Allow data inclusion flags (default False).

        Returns:
            Any: The final result from the model interaction.
        """
        # --- Helper Functions (nested for better organization) ---
        def check_stop_flags(response_text: str) -> Tuple[bool, str, str, Dict[str, bool]]:
            """Checks the model's response for loop control flags."""
            stop_flags = {
                "**// STOP_FLAG_SUCCESS //**": "success",
                "**// STOP_FLAG_FRUSTRATION_HIGH //**": "frustration",
                "**// STOP_FLAG_NO_PROGRESS //**": "no_progress",
                "**// STOP_IMMEDIATE //**": "immediate",
                "**// STOP_SIMPLE //**": "simple"
            }
            data_flags = {}
            flag_pattern = r"\*\*//\s*(INCLUDE|EXCLUDE)_(.*?)\s*//\*\*"
            for match in re.findall(flag_pattern, response_text):
                action, data_source = match
                data_flags[f"{action.lower()}_{data_source.lower()}"] = True

            for flag, reason in stop_flags.items():
                if flag in response_text:
                    return True, reason, flag, data_flags
            return False, "", "", data_flags

        def extract_text_from_response(response) -> str:
            """Extracts text content from the Gemini model's response."""
            return "".join([part.text for candidate in response.candidates
                            for part in candidate.content.parts]).strip()

        def interpret_function_calls(response, tool_manager, focus_file_path) -> Tuple[List[str], Dict]:
            """
            Interprets function calls from the model and executes them.
            Manages a shared 'context' dictionary that can be updated
            by tools and used in subsequent calls.
            """
            results = []
            context = {}
            if hasattr(response, 'candidates'):
                for candidate in response.candidates:
                    if hasattr(candidate, 'content') and hasattr(candidate.content, 'parts'):
                        for part in candidate.content.parts:
                            function_call = getattr(part, 'function_call', None)
                            if function_call:
                                self.print_colored(self.Color.MAGENTA, "---------------INTERPRETER-------------------")
                                tool_name = function_call.name

                                tool_function = tool_manager.get_tool_function(tool_name)
                                if tool_function:
                                    function_args = function_call.args
                                    self.print_colored(self.Color.YELLOW, f"Function name: {tool_name}")
                                    for key, value in function_args.items():
                                        self.print_colored(self.Color.CYAN, f"        {key}: {value}")
                                    try:
                                        result = tool_function(**function_args, context=context, focus_file_path=focus_file_path)
                                        if isinstance(result, dict) and 'context' in result:
                                            context.update(result['context'])
                                        results.append(f"Result of {tool_name}({function_args}): {result}")
                                    except Exception as e:
                                        self.print_colored(self.Color.RED, f"Error calling {tool_name}: {e}")
                                        results.append(f"Error calling {tool_name}: {e}")
                                else:
                                    self.print_colored(self.Color.RED, f"Tool function '{tool_name}' not found.")
            return results, context

        def create_session_name() -> str:
            """Creates a timestamped session name for logging."""
            return f"session_{time.strftime('%Y%m%d_%H%M%S')}"

        def handle_input_data(input_data: Dict[str, Any]) -> List:
            """Prepares and handles different input data types for the model."""
            messages = []
            for data_type, data_values in input_data.items():
                if data_type == "text":
                    if isinstance(data_values, str):
                        messages.append(data_values)
                    elif isinstance(data_values, list):
                        messages.extend(data_values)
                elif data_type in ("image", "audio"):
                    if not isinstance(data_values, list):
                        data_values = [data_values]

                    for data_value in data_values:
                        if not os.path.exists(data_value):
                            self.print_colored(self.Color.RED,
                                              f"Error: {data_type} file not found: {data_value}")
                            continue

                        try:
                            uploaded_file = genai.upload_file(path=data_value)
                            messages.append(uploaded_file)
                        except Exception as e:
                            self.print_colored(self.Color.RED,
                                              f"Error uploading {data_type}: {e}")
                else:
                    self.print_colored(self.Color.WARNING,
                                      f"Warning: Unsupported data type: {data_type}")
            return messages

        def save_data(data: Any, file_path: str):
            """Saves data to a JSON file."""
            print(f"Saving data to: {file_path}")
            try:
                with open(file_path, 'w', encoding='utf-8') as f:
                    json.dump(data, f, indent=4)
            except Exception as e:
                self.print_colored(self.Color.RED, f"Error saving data: {e}")

        def save_perception_output(perception_output, session_name, counter):
            """Saves the AI's perception output to log files."""
            output_dir = os.path.join("perception_output", session_name)
            os.makedirs(output_dir, exist_ok=True)
            file_path = os.path.join(output_dir, f"{counter}.txt")
            with open(file_path, "w", encoding='utf-8') as f:
                f.write(perception_output)

        # --- Main Logic of run_model ---
        instructions = initial_system_instruction

        if use_stop_loop_flags:
            instructions += " You can control loop execution using these flags: \
                            **// STOP_FLAG_SUCCESS //**, **// STOP_FLAG_FRUSTRATION_HIGH //**, \
                            **// STOP_FLAG_NO_PROGRESS //**, **// STOP_IMMEDIATE //**, **// STOP_SIMPLE //**."

        instructions += """ You have access to pre-loaded website data. 
                            You can manage which data types to INCLUDE or EXCLUDE in the next loop iteration using these flags:
                            **// INCLUDE_TEXT //**, **// EXCLUDE_TEXT //**, 
                            **// INCLUDE_IMAGES //**, **// EXCLUDE_IMAGES //**,
                            **// INCLUDE_LINKS //**, **// EXCLUDE_LINKS //** (and so on) """

        # Create session name and focus file path
        session_name = create_session_name()
        focus_file_path = os.path.join("focus", session_name + ".json")
        os.makedirs("focus", exist_ok=True)  # Ensure focus directory exists

        # Initialize focus
        focus = {"id": session_name}
        save_data(focus, focus_file_path)

        model = genai.GenerativeModel(
            model_name=model_name,
            safety_settings={'HARASSMENT': 'block_none'},
            system_instruction=instructions,
            tools=self.tool_manager.load_tools_of_type("all"),
            context={"focus_file_path": focus_file_path}  # Pass focus file path
        )

        model_chat = model.start_chat(history=[])
        execution_text = ""
        execution_function_calls = []
        counter = 0
        perception_output_log = ""
        short_term_memory = []
        context = {
            'web_search_data': [],
            'database_data': [],
            'web_search_query': None,
            'database_query': None,
            'load_web_search_data': False,
            'load_database_data': False
        }
        final_result = None
        current_loop = 0

        # --- Pre-loop Web Scraping ---
        website_data = {}
        if input_data and "urls" in input_data:
            if isinstance(input_data["urls"], str):
                website_data = self.scrape_website(input_data["urls"],
                                                  extract_links=True,
                                                  extract_images=True,
                                                  extract_text=True)
            elif isinstance(input_data["urls"], list):
                for url in input_data["urls"]:
                    website_data[url] = self.scrape_website(url,
                                                          extract_links=True,
                                                          extract_images=True,
                                                          extract_text=True)
        else:
            self.print_colored(self.Color.WARNING, "No URLs provided for scraping.")

        model_context = {"available_data": website_data}

        # --- Set initial data inclusion flags ---
        data_inclusion_flags = {
            "text": "INCLUDE_TEXT" in data_to_include,
            "images": "INCLUDE_IMAGES" in data_to_include,
            "links": "INCLUDE_LINKS" in data_to_include
        }

        # --- Main Interaction Loop ---
        while current_loop < max_loops and (looping or current_loop == 0):
            input_messages = []
            time.sleep(2)

            # --- User Input ---
            if enable_user_input and counter % user_input_interval == 0:
                user_input = input("Enter your input: ")
            else:
                user_input = ""

            # --- Constructing the Prompt ---
            self.print_colored(self.Color.OKGREEN,
                              f"Loop {current_loop}--------------------------------------------------")
            prompt = f"{counter}:\n"
            prompt += "system is user\n"

            # Load focus for this session
            with open(focus_file_path, "r", encoding='utf-8') as f:
                focus = json.load(f)
            if focus:
                prompt += f"Current Focus: {json.dumps(focus)}\n"
            else:
                prompt += "Current Focus: None\n"

            # Add data based on inclusion/exclusion flags
            for url, data in model_context["available_data"].items():
                if data_inclusion_flags['text'] and "text" in data:
                    prompt += f"Website Text ({url}):\n{data['text']}\n"
                if data_inclusion_flags['images'] and "images" in data:
                    prompt += f"Website Images ({url}):\n{', '.join(data['images'])}\n"
                if data_inclusion_flags['links'] and "links" in data:
                    prompt += f"Website Links ({url}):\n{', '.join(data['links'])}\n"

            # Inject additional prompts if provided
            if injection_prompts:
                input_messages.extend(injection_prompts)

            # Add short-term memory (recent interactions) to the prompt
            if short_term_memory:
                prompt += "Recent Interactions:\n"
                for i, memory_item in enumerate(short_term_memory):
                    prompt += f"  - {memory_item}\n"

            prompt += user_input

            # Combine text and media messages for the model
            input_messages.insert(0, prompt)

            # --- Model Interaction ---
            try:
                print("Sending message...")
                response = model_chat.send_message(input_messages)
                try:
                    execution_text = extract_text_from_response(response)
                except Exception as e:
                    print(e)
                    execution_text="..."
                try:
                    execution_function_calls, context = interpret_function_calls(response, self.tool_manager, focus_file_path)
                except Exception as e:
                    print(e)
                    execution_function_calls, context=""

                # Check for stop flags in the response
                should_stop, stop_reason, stop_flag, _ = check_stop_flags(execution_text)
                if should_stop:
                    self.print_colored(self.Color.WARNING,
                                      f"Stopping loop due to flag: {stop_flag} ({stop_reason})")
                    break

                # Update data inclusion/exclusion flags based on the model's response
                if use_data_loading_flags:
                    data_flag_mapping = {
                        "**// INCLUDE_TEXT //**": "text",
                        "**// EXCLUDE_TEXT //**": "text",
                        "**// INCLUDE_IMAGES //**": "images",
                        "**// EXCLUDE_IMAGES //**": "images",
                        "**// INCLUDE_LINKS //**": "links",
                        "**// EXCLUDE_LINKS //**": "links",
                    }

                    for flag, data_type in data_flag_mapping.items():
                        if flag in execution_text:
                            data_inclusion_flags[data_type] = "INCLUDE" in flag

                # Output interpretation based on expected type
                if expected_output_type == "json":
                    try:
                        output_data = json.loads(execution_text)
                        print(f"Parsed JSON Output: {output_data}")
                        final_result = output_data
                    except json.JSONDecodeError:
                        self.print_colored(self.Color.RED, "Error: Model output is not valid JSON.")
                else:
                    self.print_colored(self.Color.OKBLUE, f" Response: {execution_text}")
                    final_result = execution_text

                self.print_colored(self.Color.OKCYAN, f" Function Calls: {execution_function_calls}")

                # Log perception output
                perception_output_log += (
                    f"\n{self.Color.OKGREEN}Prompt: {self.Color.ENDC}{prompt}"
                    f"\n{self.Color.OKBLUE}Response: {self.Color.ENDC}{execution_text}"
                    f"\n{self.Color.OKCYAN}Function Calls: {self.Color.ENDC}{execution_function_calls}"
                )
                save_perception_output(perception_output_log, session_name, counter)

                # Update short-term memory
                short_term_memory.append(f"User: {user_input}")
                short_term_memory.append(f"Assistant: {execution_text}")

            except Exception as e:
                self.print_colored(self.Color.RED, f"Error in loop: {e}")

            current_loop += 1

        self.print_colored(self.Color.OKGREEN, "Exiting the loop.")
        return final_result

# Usage example:
if __name__ == "__main__":
    runner = Geuron_Gemini()
    urls_to_scrape = [
        "https://www.example.com",
        "https://www.wikipedia.org"
    ]

    result = runner.run_model(
        model_name="gemini-pro",
        initial_system_instruction="You are a helpful AI assistant that analyzes websites.",
        data_to_include=["text", "images", "links"],
        input_data={"urls": urls_to_scrape},
        expected_output_type="text",
        enable_user_input=True,
        max_loops=3,
        use_data_loading_flags=True
    )
    print(f"Final Result:\n{result}")

File: keys.py (C:\Users\DELL\Desktop\openAIF_frontend\OctopusAI\PROJECT\SMART_BOT4\keys.py)
Content (First 1 lines):
googleKey='your  key'

File: summarisation.txt (C:\Users\DELL\Desktop\openAIF_frontend\OctopusAI\PROJECT\SMART_BOT4\summarisation.txt)
Content (First 491 lines):
## Summary of Files and Directories in 'C:\Users\DELL\Desktop\openAIF_frontend\OctopusAI\PROJECT\SMART_BOT4'

File: focus.json (C:\Users\DELL\Desktop\openAIF_frontend\OctopusAI\PROJECT\SMART_BOT4\focus.json)
Content (First 25 lines):
{
  "focus": [
    {
      "category": "Research",
      "text": "....",
      "frustration_level": 2,
      "focus_strength": 8,
      "defocus_threshold": 5
    },
    {
      "category": "Task",
      "text": "....",
      "frustration_level": 1,
      "focus_strength": 7,
      "defocus_threshold": 4
    },
    {
      "category": "Goal",
      "text": "...",
      "frustration_level": 0,
      "focus_strength": 9,
      "defocus_threshold": 3
    }
  ]
}

File: GeminiModelRunner_Perceptron.py (C:\Users\DELL\Desktop\openAIF_frontend\OctopusAI\PROJECT\SMART_BOT4\GeminiModelRunner_Perceptron.py)
Content (First 458 lines):
import time
import os
import json
import re
import google.generativeai as genai
from typing import Dict, List, Any, Tuple
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.common.exceptions import TimeoutException

# Assuming these modules exist and are correctly implemented:
from TOOL_MANAGER import ToolManager
from tools.ai.update_focus import update_focus

# Load Google Gemini API key
from keys import googleKey as API_KEY

class Geuron_Gemini:
    """
    A class to manage interactions with the Google Gemini model,
    including web scraping, prompt construction, tool execution,
    and response processing.
    """
    def __init__(self):
        self.tools_folder = "tools"
        self.tool_manager = ToolManager(self.tools_folder)
        genai.configure(api_key=API_KEY)

    class Color:
        """ANSI color codes for enhanced console output."""
        HEADER = '\033[95m'
        OKBLUE = '\033[94m'
        OKCYAN = '\033[96m'
        OKGREEN = '\033[92m'
        WARNING = '\033[93m'
        FAIL = '\033[91m'
        ENDC = '\033[0m'  # Resets color
        BOLD = '\033[1m'
        UNDERLINE = '\033[4m'
        PURPLE = '\033[95m'
        MAGENTA = '\033[35m'
        YELLOW = '\033[33m'
        CYAN = '\033[36m'
        RED = '\033[31m'

    def print_colored(self, color, text):
        """Prints text with the specified ANSI color."""
        print(color + text + self.Color.ENDC)

    def scrape_website(self, url: str, extract_links: bool = True,
                       extract_images: bool = True,
                       extract_text: bool = True) -> Dict[str, Any]:
        """
        Scrapes data from a website using Selenium.

        Args:
            url (str): Website URL to scrape.
            extract_links (bool): Extract links (default True).
            extract_images (bool): Extract image URLs (default True).
            extract_text (bool): Extract text content (default True).

        Returns:
            Dict[str, Any]: Scraped data ('links', 'images', 'text').
        """
        print(f"Scraping website: {url}")
        scraped_data = {}

        options = webdriver.ChromeOptions()
        # options.add_argument('--headless=new') # Uncomment for headless mode
        driver = webdriver.Chrome(options=options)
        driver.get(url)

        try:
            WebDriverWait(driver, 10).until(
                EC.presence_of_element_located((By.TAG_NAME, "body"))
            )

            if extract_links:
                scraped_data['links'] = [link.get_attribute("href")
                                          for link in driver.find_elements(By.TAG_NAME, "a")]

            if extract_images:
                scraped_data['images'] = [img.get_attribute("src")
                                          for img in driver.find_elements(By.TAG_NAME, "img")]

            if extract_text:
                scraped_data['text'] = driver.find_element(By.TAG_NAME, "body").text

        except TimeoutException:
            self.print_colored(self.Color.RED,
                              f"Timeout: Page loading took too long for {url}")
        except Exception as e:
            self.print_colored(self.Color.RED, f"Web scraping error: {e}")

        driver.quit()
        return scraped_data

    def run_model(self,
                  model_name: str,
                  initial_system_instruction: str = "You are a helpful AI assistant.",
                  use_stop_loop_flags: bool = False,
                  enable_user_input: bool = False,
                  user_input_interval: int = 15,
                  max_loops: int = 10,
                  looping: bool = True,
                  data_to_include: List[str] = ["text"],
                  injection_prompts: List[str] = None,
                  input_data: Dict[str, Any] = None,
                  expected_output_type: str = None,
                  use_data_loading_flags: bool = False) -> Any:
        """
        Runs the Google Gemini model, manages the interaction loop,
        processes responses, and handles tool executions.

        Args:
            model_name (str): Name of the Gemini model to use.
            initial_system_instruction (str): Initial instructions for the model.
            use_stop_loop_flags (bool): Allow loop control with flags (default False).
            enable_user_input (bool): Enable user input during the loop (default False).
            user_input_interval (int): How often to prompt for user input (default 15).
            max_loops (int): Maximum number of interaction loops (default 10).
            looping (bool): Run in a loop (default True).
            data_to_include (List[str]): Initial data types to include ('text', 'images', 'links').
            injection_prompts (List[str]): Additional prompts to inject.
            input_data (Dict[str, Any]): Input data for the model ('urls', etc.).
            expected_output_type (str): Expected output type ('json', 'text', etc.).
            use_data_loading_flags (bool): Allow data inclusion flags (default False).

        Returns:
            Any: The final result from the model interaction.
        """
        # --- Helper Functions (nested for better organization) ---
        def check_stop_flags(response_text: str) -> Tuple[bool, str, str, Dict[str, bool]]:
            """Checks the model's response for loop control flags."""
            stop_flags = {
                "**// STOP_FLAG_SUCCESS //**": "success",
                "**// STOP_FLAG_FRUSTRATION_HIGH //**": "frustration",
                "**// STOP_FLAG_NO_PROGRESS //**": "no_progress",
                "**// STOP_IMMEDIATE //**": "immediate",
                "**// STOP_SIMPLE //**": "simple"
            }
            data_flags = {}
            flag_pattern = r"\*\*//\s*(INCLUDE|EXCLUDE)_(.*?)\s*//\*\*"
            for match in re.findall(flag_pattern, response_text):
                action, data_source = match
                data_flags[f"{action.lower()}_{data_source.lower()}"] = True

            for flag, reason in stop_flags.items():
                if flag in response_text:
                    return True, reason, flag, data_flags
            return False, "", "", data_flags

        def extract_text_from_response(response) -> str:
            """Extracts text content from the Gemini model's response."""
            return "".join([part.text for candidate in response.candidates
                            for part in candidate.content.parts]).strip()

        def interpret_function_calls(response, tool_manager, focus_file_path) -> Tuple[List[str], Dict]:
            """
            Interprets function calls from the model and executes them.
            Manages a shared 'context' dictionary that can be updated
            by tools and used in subsequent calls.
            """
            results = []
            context = {}
            if hasattr(response, 'candidates'):
                for candidate in response.candidates:
                    if hasattr(candidate, 'content') and hasattr(candidate.content, 'parts'):
                        for part in candidate.content.parts:
                            function_call = getattr(part, 'function_call', None)
                            if function_call:
                                self.print_colored(self.Color.MAGENTA, "---------------INTERPRETER-------------------")
                                tool_name = function_call.name

                                tool_function = tool_manager.get_tool_function(tool_name)
                                if tool_function:
                                    function_args = function_call.args
                                    self.print_colored(self.Color.YELLOW, f"Function name: {tool_name}")
                                    for key, value in function_args.items():
                                        self.print_colored(self.Color.CYAN, f"        {key}: {value}")
                                    try:
                                        result = tool_function(**function_args, context=context, focus_file_path=focus_file_path)
                                        if isinstance(result, dict) and 'context' in result:
                                            context.update(result['context'])
                                        results.append(f"Result of {tool_name}({function_args}): {result}")
                                    except Exception as e:
                                        self.print_colored(self.Color.RED, f"Error calling {tool_name}: {e}")
                                        results.append(f"Error calling {tool_name}: {e}")
                                else:
                                    self.print_colored(self.Color.RED, f"Tool function '{tool_name}' not found.")
            return results, context

        def create_session_name() -> str:
            """Creates a timestamped session name for logging."""
            return f"session_{time.strftime('%Y%m%d_%H%M%S')}"

        def handle_input_data(input_data: Dict[str, Any]) -> List:
            """Prepares and handles different input data types for the model."""
            messages = []
            for data_type, data_values in input_data.items():
                if data_type == "text":
                    if isinstance(data_values, str):
                        messages.append(data_values)
                    elif isinstance(data_values, list):
                        messages.extend(data_values)
                elif data_type in ("image", "audio"):
                    if not isinstance(data_values, list):
                        data_values = [data_values]

                    for data_value in data_values:
                        if not os.path.exists(data_value):
                            self.print_colored(self.Color.RED,
                                              f"Error: {data_type} file not found: {data_value}")
                            continue

                        try:
                            uploaded_file = genai.upload_file(path=data_value)
                            messages.append(uploaded_file)
                        except Exception as e:
                            self.print_colored(self.Color.RED,
                                              f"Error uploading {data_type}: {e}")
                else:
                    self.print_colored(self.Color.WARNING,
                                      f"Warning: Unsupported data type: {data_type}")
            return messages

        def save_data(data: Any, file_path: str):
            """Saves data to a JSON file."""
            print(f"Saving data to: {file_path}")
            try:
                with open(file_path, 'w', encoding='utf-8') as f:
                    json.dump(data, f, indent=4)
            except Exception as e:
                self.print_colored(self.Color.RED, f"Error saving data: {e}")

        def save_perception_output(perception_output, session_name, counter):
            """Saves the AI's perception output to log files."""
            output_dir = os.path.join("perception_output", session_name)
            os.makedirs(output_dir, exist_ok=True)
            file_path = os.path.join(output_dir, f"{counter}.txt")
            with open(file_path, "w", encoding='utf-8') as f:
                f.write(perception_output)

        # --- Main Logic of run_model ---
        instructions = initial_system_instruction

        if use_stop_loop_flags:
            instructions += " You can control loop execution using these flags: \
                            **// STOP_FLAG_SUCCESS //**, **// STOP_FLAG_FRUSTRATION_HIGH //**, \
                            **// STOP_FLAG_NO_PROGRESS //**, **// STOP_IMMEDIATE //**, **// STOP_SIMPLE //**."

        instructions += """ You have access to pre-loaded website data. 
                            You can manage which data types to INCLUDE or EXCLUDE in the next loop iteration using these flags:
                            **// INCLUDE_TEXT //**, **// EXCLUDE_TEXT //**, 
                            **// INCLUDE_IMAGES //**, **// EXCLUDE_IMAGES //**,
                            **// INCLUDE_LINKS //**, **// EXCLUDE_LINKS //** (and so on) """

        # Create session name and focus file path
        session_name = create_session_name()
        focus_file_path = os.path.join("focus", session_name + ".json")
        os.makedirs("focus", exist_ok=True)  # Ensure focus directory exists

        # Initialize focus
        focus = {"id": session_name}
        save_data(focus, focus_file_path)

        model = genai.GenerativeModel(
            model_name=model_name,
            safety_settings={'HARASSMENT': 'block_none'},
            system_instruction=instructions,
            tools=self.tool_manager.load_tools_of_type("all"),
            context={"focus_file_path": focus_file_path}  # Pass focus file path
        )

        model_chat = model.start_chat(history=[])
        execution_text = ""
        execution_function_calls = []
        counter = 0
        perception_output_log = ""
        short_term_memory = []
        context = {
            'web_search_data': [],
            'database_data': [],
            'web_search_query': None,
            'database_query': None,
            'load_web_search_data': False,
            'load_database_data': False
        }
        final_result = None
        current_loop = 0

        # --- Pre-loop Web Scraping ---
        website_data = {}
        if input_data and "urls" in input_data:
            if isinstance(input_data["urls"], str):
                website_data = self.scrape_website(input_data["urls"],
                                                  extract_links=True,
                                                  extract_images=True,
                                                  extract_text=True)
            elif isinstance(input_data["urls"], list):
                for url in input_data["urls"]:
                    website_data[url] = self.scrape_website(url,
                                                          extract_links=True,
                                                          extract_images=True,
                                                          extract_text=True)
        else:
            self.print_colored(self.Color.WARNING, "No URLs provided for scraping.")

        model_context = {"available_data": website_data}

        # --- Set initial data inclusion flags ---
        data_inclusion_flags = {
            "text": "INCLUDE_TEXT" in data_to_include,
            "images": "INCLUDE_IMAGES" in data_to_include,
            "links": "INCLUDE_LINKS" in data_to_include
        }

        # --- Main Interaction Loop ---
        while current_loop < max_loops and (looping or current_loop == 0):
            input_messages = []
            time.sleep(2)

            # --- User Input ---
            if enable_user_input and counter % user_input_interval == 0:
                user_input = input("Enter your input: ")
            else:
                user_input = ""

            # --- Constructing the Prompt ---
            self.print_colored(self.Color.OKGREEN,
                              f"Loop {current_loop}--------------------------------------------------")
            prompt = f"{counter}:\n"
            prompt += "system is user\n"

            # Load focus for this session
            with open(focus_file_path, "r", encoding='utf-8') as f:
                focus = json.load(f)
            if focus:
                prompt += f"Current Focus: {json.dumps(focus)}\n"
            else:
                prompt += "Current Focus: None\n"

            # Add data based on inclusion/exclusion flags
            for url, data in model_context["available_data"].items():
                if data_inclusion_flags['text'] and "text" in data:
                    prompt += f"Website Text ({url}):\n{data['text']}\n"
                if data_inclusion_flags['images'] and "images" in data:
                    prompt += f"Website Images ({url}):\n{', '.join(data['images'])}\n"
                if data_inclusion_flags['links'] and "links" in data:
                    prompt += f"Website Links ({url}):\n{', '.join(data['links'])}\n"

            # Inject additional prompts if provided
            if injection_prompts:
                input_messages.extend(injection_prompts)

            # Add short-term memory (recent interactions) to the prompt
            if short_term_memory:
                prompt += "Recent Interactions:\n"
                for i, memory_item in enumerate(short_term_memory):
                    prompt += f"  - {memory_item}\n"

            prompt += user_input

            # Combine text and media messages for the model
            input_messages.insert(0, prompt)

            # --- Model Interaction ---
            try:
                print("Sending message...")
                response = model_chat.send_message(input_messages)
                try:
                    execution_text = extract_text_from_response(response)
                except Exception as e:
                    print(e)
                    execution_text="..."
                try:
                    execution_function_calls, context = interpret_function_calls(response, self.tool_manager, focus_file_path)
                except Exception as e:
                    print(e)
                    execution_function_calls, context=""

                # Check for stop flags in the response
                should_stop, stop_reason, stop_flag, _ = check_stop_flags(execution_text)
                if should_stop:
                    self.print_colored(self.Color.WARNING,
                                      f"Stopping loop due to flag: {stop_flag} ({stop_reason})")
                    break

                # Update data inclusion/exclusion flags based on the model's response
                if use_data_loading_flags:
                    data_flag_mapping = {
                        "**// INCLUDE_TEXT //**": "text",
                        "**// EXCLUDE_TEXT //**": "text",
                        "**// INCLUDE_IMAGES //**": "images",
                        "**// EXCLUDE_IMAGES //**": "images",
                        "**// INCLUDE_LINKS //**": "links",
                        "**// EXCLUDE_LINKS //**": "links",
                    }

                    for flag, data_type in data_flag_mapping.items():
                        if flag in execution_text:
                            data_inclusion_flags[data_type] = "INCLUDE" in flag

                # Output interpretation based on expected type
                if expected_output_type == "json":
                    try:
                        output_data = json.loads(execution_text)
                        print(f"Parsed JSON Output: {output_data}")
                        final_result = output_data
                    except json.JSONDecodeError:
                        self.print_colored(self.Color.RED, "Error: Model output is not valid JSON.")
                else:
                    self.print_colored(self.Color.OKBLUE, f" Response: {execution_text}")
                    final_result = execution_text

                self.print_colored(self.Color.OKCYAN, f" Function Calls: {execution_function_calls}")

                # Log perception output
                perception_output_log += (
                    f"\n{self.Color.OKGREEN}Prompt: {self.Color.ENDC}{prompt}"
                    f"\n{self.Color.OKBLUE}Response: {self.Color.ENDC}{execution_text}"
                    f"\n{self.Color.OKCYAN}Function Calls: {self.Color.ENDC}{execution_function_calls}"
                )
                save_perception_output(perception_output_log, session_name, counter)

                # Update short-term memory
                short_term_memory.append(f"User: {user_input}")
                short_term_memory.append(f"Assistant: {execution_text}")

            except Exception as e:
                self.print_colored(self.Color.RED, f"Error in loop: {e}")

            current_loop += 1

        self.print_colored(self.Color.OKGREEN, "Exiting the loop.")
        return final_result

# Usage example:
if __name__ == "__main__":
    runner = Geuron_Gemini()
    urls_to_scrape = [
        "https://www.example.com",
        "https://www.wikipedia.org"
    ]

    result = runner.run_model(
        model_name="gemini-pro",
        initial_system_instruction="You are a helpful AI assistant that analyzes websites.",
        data_to_include=["text", "images", "links"],
        input_data={"urls": urls_to_scrape},
        expected_output_type="text",
        enable_user_input=True,
        max_loops=3,
        use_data_loading_flags=True
    )
    print(f"Final Result:\n{result}")




Subdirectory: tools
## Summary of Files and Directories in 'C:\Users\DELL\Desktop\openAIF_frontend\OctopusAI\PROJECT\SMART_BOT4\tools'


Subdirectory: ai
## Summary of Files and Directories in 'C:\Users\DELL\Desktop\openAIF_frontend\OctopusAI\PROJECT\SMART_BOT4\tools\ai'

File: update_focus.py (C:\Users\DELL\Desktop\openAIF_frontend\OctopusAI\PROJECT\SMART_BOT4\tools\ai\update_focus.py)
Content (First 67 lines):
tool_type_for_TOOL_MANAGER="all"


update_focus_short_description=""" Updates the focus file with new focus information.. 
        """



import json
import os


# Path to the focus file (adjust if needed)
focus_file_path = '../../focus.json'

def update_focus(new_focus: str, category: str = None, frustration_level: int = None, focus_strength: int = None, defocus_threshold: int = None) -> dict:
  """
  Updates the focus file with new focus information.

  Args:
    new_focus (str): The new focus text to be added to the focus file.
    category (str, optional): The category of the focus (e.g., "Research", "Task", "Goal"). Defaults to None.
    frustration_level (int, optional): A level indicating the current frustration level (0-10). Defaults to None.
    focus_strength (int, optional): A level indicating the strength of the focus (0-10). Defaults to None.
    defocus_threshold (int, optional): A level indicating the threshold at which the focus should be considered defocused (0-10). Defaults to None.

  Returns:
    dict: A dictionary containing the status of the operation, a message, and the updated focus text.
  """

  try:
    # Read the existing focus from the file
    with open(focus_file_path, 'r') as f:
      focus_data = json.load(f)

    # Create a new focus item dictionary
    new_focus_item = {
      "text": new_focus,
      "category": category,
      "frustration_level": frustration_level,
      "focus_strength": focus_strength,
      "defocus_threshold": defocus_threshold
    }

    # Append the new focus item to the existing focus list
    focus_data['focus'].append(new_focus_item)

    # Write the updated focus back to the file
    with open(focus_file_path, 'w') as f:
      json.dump(focus_data, f, indent=4)

    return {
      "status": "success",
      "message": f"Focus updated with: '{new_focus}'",
      "updated_focus": focus_data['focus']
    }

  except Exception as e:
    return {
      "status": "failure",
      "message": f"Error updating focus: {str(e)}"
    }

# Example usage:
# new_focus_text = "My new focus is to learn more about programming."
# result = update_focus(new_focus_text, category="Goal", frustration_level=0, focus_strength=9, defocus_threshold=3)
# print(result)


Subdirectory: __pycache__
## Summary of Files and Directories in 'C:\Users\DELL\Desktop\openAIF_frontend\OctopusAI\PROJECT\SMART_BOT4\tools\ai\__pycache__'

File: update_focus.cpython-312.pyc (C:\Users\DELL\Desktop\openAIF_frontend\OctopusAI\PROJECT\SMART_BOT4\tools\ai\__pycache__\update_focus.cpython-312.pyc)
Error decoding file 'C:\Users\DELL\Desktop\openAIF_frontend\OctopusAI\PROJECT\SMART_BOT4\tools\ai\__pycache__\update_focus.cpython-312.pyc': 'utf-8' codec can't decode byte 0xcb in position 0: invalid continuation byte


Subdirectory: os
## Summary of Files and Directories in 'C:\Users\DELL\Desktop\openAIF_frontend\OctopusAI\PROJECT\SMART_BOT4\tools\os'

File: tool_read_from_file.py (C:\Users\DELL\Desktop\openAIF_frontend\OctopusAI\PROJECT\SMART_BOT4\tools\os\tool_read_from_file.py)
Content (First 22 lines):
tool_type_for_TOOL_MANAGER="os"


tool_read_from_file_short_description=""" Reads content from a file. 
        """

def tool_read_from_file(file_path: str) -> str:
    """
    Reads content from a file.

    Args:
        file_path (str): The path to the file to be read.

    Returns:
        str: The content of the file, or an error message if the file cannot be read.
    """
    try:
        with open(file_path, 'r', encoding='utf-8') as f:
            content = f.read()
        return content
    except Exception as e:
        return f"Error reading file: {str(e)}"

File: tool_save_to_file.py (C:\Users\DELL\Desktop\openAIF_frontend\OctopusAI\PROJECT\SMART_BOT4\tools\os\tool_save_to_file.py)
Content (First 46 lines):
tool_type_for_TOOL_MANAGER="os"
tool_save_to_file_short_description="Saves content to a file with the specified name and path."
import  os


def tool_save_to_file(content: str = None, file_name: str = 'NoName', file_path: str = None, encoding: str = 'utf-8', create_folders: bool = True) -> dict:
    """
    Saves content to a file with the specified name and path.

    Args:
        content (str, optional): The content to be written to the file. Defaults to None, which will write an empty string.
        file_name (str, optional): The name of the file to be created. Defaults to 'NoName'.
        file_path (str, optional): The path to the directory where the file should be created. If None, the current working directory will be used. Defaults to None.
        encoding (str, optional): The encoding to use for the file. Defaults to 'utf-8'.
        create_folders (bool, optional): Whether to create missing folders in the file path. Defaults to True.

    Returns:
        dict: A dictionary containing the status of the operation, a message, and the full path to the file.
    """

    print(f"Entering: save_to_file(...)", 'blue')
    if content is None:
        content = ""
    if file_path is None:
        full_path = os.path.join(os.getcwd(), file_name)
    else:
        full_path = os.path.join(file_path, file_name)

    # Create folders if they don't exist
    if create_folders:
        os.makedirs(os.path.dirname(full_path), exist_ok=True)

    try:
        with open(full_path, 'w', encoding=encoding) as f:
            f.write(content)

        success_message = f"File saved successfully at: {full_path}"
        print(success_message, 'green')
        print(f"Exiting: save_to_file(...)", 'blue')
        return {"status": "success", "message": success_message, "file_path": full_path}

    except Exception as e:
        error_message = f"Failed to save file: {str(e)}"
        print(error_message, 'red')
        print(f"Exiting: save_to_file(...)", 'blue')
        return {"status": "failure", "message": error_message}


Subdirectory: __pycache__
## Summary of Files and Directories in 'C:\Users\DELL\Desktop\openAIF_frontend\OctopusAI\PROJECT\SMART_BOT4\tools\os\__pycache__'

File: tool_read_from_file.cpython-312.pyc (C:\Users\DELL\Desktop\openAIF_frontend\OctopusAI\PROJECT\SMART_BOT4\tools\os\__pycache__\tool_read_from_file.cpython-312.pyc)
Error decoding file 'C:\Users\DELL\Desktop\openAIF_frontend\OctopusAI\PROJECT\SMART_BOT4\tools\os\__pycache__\tool_read_from_file.cpython-312.pyc': 'utf-8' codec can't decode byte 0xcb in position 0: invalid continuation byte

File: tool_save_to_file.cpython-312.pyc (C:\Users\DELL\Desktop\openAIF_frontend\OctopusAI\PROJECT\SMART_BOT4\tools\os\__pycache__\tool_save_to_file.cpython-312.pyc)
Error decoding file 'C:\Users\DELL\Desktop\openAIF_frontend\OctopusAI\PROJECT\SMART_BOT4\tools\os\__pycache__\tool_save_to_file.cpython-312.pyc': 'utf-8' codec can't decode byte 0xcb in position 0: invalid continuation byte


Subdirectory: web
## Summary of Files and Directories in 'C:\Users\DELL\Desktop\openAIF_frontend\OctopusAI\PROJECT\SMART_BOT4\tools\web'

File: scrape_web.py (C:\Users\DELL\Desktop\openAIF_frontend\OctopusAI\PROJECT\SMART_BOT4\tools\web\scrape_web.py)
Content (First 515 lines):
tool_type_for_TOOL_MANAGER="all"


scrape_web_short_description=""" scrapes web. 
        """



import json
import urllib
from googlesearch import search
import random
import requests
from requests.exceptions import SSLError, TimeoutException
from bs4 import BeautifulSoup
from urllib.parse import urljoin, urlparse, urlunparse
import time
import re
from selenium import webdriver
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.common.by import By
from selenium.webdriver.chrome.options import Options
import os
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
import socket
import datetime

# Global variables for storing gathered data
SetUrlsGlobal = set()
SetLinksGlobal = set()
SetImagesGlobal = set()

# Default configuration
DEFAULT_CONFIG = {
    "source": "google",
    "query": None,
    "initial_processing_method": "random",
    "initial_filtering_phrases": [],
    "excluded_phrases": [],
    "image_extraction_method": "selenium",
    "save_structure": "folder",  # Options: 'folder', 'flat', 'json'
    "save_path": "scraped_data",
    "max_depth": 3,
    "rate_limit": 1,  # Seconds between requests
}


def scrape_web(source=None, query=None, initial_processing_method=None, initial_filtering_phrases=[],
               excluded_phrases=[], image_extraction_method=None, save_structure=None, save_path=None, max_depth=None,
               rate_limit=None):
    """

    The main function to scrape the web.

    This function orchestrates the entire web scraping process, from obtaining initial links to saving extracted data. It utilizes various methods for search, link processing, filtering, image extraction, and data storage, providing a comprehensive and customizable solution.

    Args:
        source (str, optional): The search engine to use for obtaining initial links ("google" or "duckduckgo"). Defaults to None, which will use the value specified in `DEFAULT_CONFIG`.
        query (str, optional): The search query to use for retrieving initial links. Defaults to None, which will use the value specified in `DEFAULT_CONFIG`.
        initial_processing_method (str, optional): The method to process the initial set of links ("random", "switch", "random_number"). Defaults to None, which will use the value specified in `DEFAULT_CONFIG`.
        initial_filtering_phrases (list, optional): A list of phrases that links should contain to be included in the initial set. Defaults to [], which will use the value specified in `DEFAULT_CONFIG`.
        excluded_phrases (list, optional): A list of phrases that should be excluded from links and images during crawling. Defaults to [], which will use the value specified in `DEFAULT_CONFIG`.
        image_extraction_method (str, optional): The method to extract images ("selenium" or "bs4"). Defaults to None, which will use the value specified in `DEFAULT_CONFIG`.
        save_structure (str, optional): The structure to save the scraped data ("folder", "flat", "json"). Defaults to None, which will use the value specified in `DEFAULT_CONFIG`.
        save_path (str, optional): The directory where the scraped data will be saved. Defaults to None, which will use the value specified in `DEFAULT_CONFIG`.
        max_depth (int, optional): The maximum number of levels to crawl (starting from the initial links). Defaults to None, which will use the value specified in `DEFAULT_CONFIG`.
        rate_limit (int, optional): The delay in seconds between requests to avoid overloading the target website. Defaults to None, which will use the value specified in `DEFAULT_CONFIG`.

    Returns:
        None: This function does not return a value but performs the web scraping process.
    """


    def resolve_ip_address(url):
        """Resolves the IP address of a given URL."""
        parsed_url = urlparse(url)
        domain = parsed_url.netloc
        try:
            ip_address = socket.gethostbyname(domain)
            return ip_address
        except socket.gaierror:
            return None

    def filter_link(link, excluded_phrases):
        """Filters a link based on excluded phrases and image extensions."""
        image_extensions = [".jpeg", ".jpg", ".gif", ".png"]

        # Exclude image links
        for extension in image_extensions:
            if link.endswith(extension):
                return False

        # Exclude links containing excluded phrases
        for phrase in excluded_phrases:
            if phrase.lower() in link.lower():
                return False

        return True

    def process_initial_set(resultSet, method="random"):
        """Processes the initial set of links based on the chosen method."""
        finalSet = resultSet.copy()
        if method == "random":
            finalSet = set(random.sample(finalSet, len(finalSet)))
        elif method == "switch":
            finalSet = set(list(finalSet)[::-1])
        elif method == "random_number":
            num_to_keep = int(input("Enter the number of entries to keep: "))
            finalSet = set(random.sample(finalSet, num_to_keep))
        return finalSet

    def get_initial_links(source="google", query=None):
        """Retrieves initial links from Google or DuckDuckGo."""
        initialLinks = set()

        if source == "google":
            if query is None:
                print("Please provide a search query for Google.")
                return initialLinks
            num_results = int(input("Enter the number of links to obtain from Google: "))
            search_results = search(query, num_results=num_results)
            initialLinks = set(search_results)
        elif source == "duckduckgo":
            driver = webdriver.Chrome()
            driver.get("https://duckduckgo.com/")

            def perform_search(driver, search_phrase):
                search_input = driver.find_element(By.NAME, "q")
                search_input.send_keys(search_phrase)
                search_input.submit()

            def get_search_result_links(driver):
                try:
                    # Wait for the search results container to be present (adjust the selector if needed)
                    results_container = WebDriverWait(driver, 10).until(
                        EC.presence_of_element_located((By.ID, "search-results"))
                    )

                    # Then, wait for links within the container to appear:
                    search_results = WebDriverWait(driver, 10).until(
                        EC.presence_of_all_elements_located((By.CSS_SELECTOR, "#search-results a[href]"))
                    )
                    links = [
                        link.get_attribute("href")
                        for link in search_results
                        if "duckduckgo" not in link.get_attribute("href")
                    ]
                    return links
                except TimeoutException:
                    print("Timeout waiting for search results. Proceeding with an empty link list.")
                    return []

            search_phrase = input("Enter DuckDuckGo search phrase: ")
            num_more_results = int(input("Number of 'More Results' scrolls: "))
            perform_search(driver, search_phrase)

            while num_more_results > 0:
                try:
                    num_more_results -= 1
                    more_results_button = WebDriverWait(driver, 3).until(
                        EC.element_to_be_clickable((By.ID, "more-results"))
                    )
                    more_results_button.click()
                except:
                    print("Failed to click the 'More Results' button.")
                    print(num_more_results)

            initialLinks = set(get_search_result_links(driver))
            driver.quit()
        else:
            print("Invalid source specified. Please use 'google' or 'duckduckgo'.")

        return initialLinks

    def filter_initial_links(initialLinks, phrases):
        """Filters initial links based on user-provided phrases."""
        filtered_links = set()
        if phrases:
            filtered_links = {
                link for link in initialLinks if any(phrase in link for phrase in phrases)
            }
        else:
            filtered_links = initialLinks
        return filtered_links

    def crawl_links(
            starting_links,
            visited_links=None,
            layer=None,
            excluded_phrases=None,
            image_extraction_method="selenium",
            config=DEFAULT_CONFIG,
    ):
        """Crawls links and extracts data based on provided parameters."""
        if visited_links is None:
            visited_links = set()

        for link in starting_links:
            if link not in visited_links:
                print(f"Crawling link: {link}")
                try:
                    response = requests.get(link)
                    response.raise_for_status()  # Raise an exception for HTTP errors
                    soup = BeautifulSoup(response.text, "html.parser")
                    ip_address = resolve_ip_address(link)

                    new_links = set()
                    images = set()

                    # Image extraction
                    if image_extraction_method == "selenium":
                        images = extract_images_selenium(link, soup)
                    elif image_extraction_method == "bs4":
                        images = extract_images_bs4(link, soup)
                    else:
                        print(
                            "Invalid image extraction method. Please use 'selenium' or 'bs4'."
                        )

                    # Link extraction
                    for tag in soup.find_all(["a", "img", "ul", "li", "div", "body"]):
                        if tag.name == "a":
                            href = tag.get("href")
                        elif tag.name == "img":
                            href = tag.get("src")
                        elif tag.name in ["ul", "li", "div", "body"]:
                            href = tag.get("data-href")  # Adjust attribute if needed
                        if href and href.startswith("http"):
                            if filter_link(href, excluded_phrases):
                                new_links.add(href)
                                if href not in visited_links:
                                    save_data(
                                        href,
                                        ip_address,
                                        layer,
                                        "links",
                                        config,
                                    )
                        else:
                            full_link = urljoin(str(link), str(href))
                            if full_link.startswith("http") and filter_link(
                                    full_link, excluded_phrases
                            ):
                                new_links.add(full_link)
                                if full_link not in visited_links:
                                    save_data(
                                        full_link,
                                        ip_address,
                                        layer,
                                        "links",
                                        config,
                                    )

                    # Save extracted images
                    if images:
                        for image_link in images:
                            save_data(
                                image_link, None, layer, "images", config
                            )

                    # Recursively crawl new links
                    if layer is not None and layer < config["max_depth"]:
                        crawl_links(
                            new_links,
                            visited_links,
                            layer + 1,
                            excluded_phrases,
                            image_extraction_method,
                            config,
                        )
                    time.sleep(config["rate_limit"])
                except requests.exceptions.RequestException as e:
                    print(f"Error crawling link: {link}, reason: {e}")
                except Exception as e:
                    print(f"An unexpected error occurred: {e}")
                finally:
                    visited_links.add(link)
            else:
                print(f"Link {link} has already been visited.")

    def extract_images_selenium(link, soup):
        """Extracts image links using Selenium."""
        images = set()
        try:
            options = Options()
            options.add_argument("--headless")
            driver = webdriver.Chrome(options=options)
            driver.get(link)

            # Wait for page to load
            WebDriverWait(driver, 10).until(EC.presence_of_element_located((By.TAG_NAME, "body")))

            # Extract image links
            for tag in driver.find_elements(By.XPATH, "//img"):
                image_url = tag.get_attribute("src")
                if image_url and image_url.startswith("http"):
                    alt_description = tag.get_attribute("alt")
                    image_source = "src"
                    formatted_image_link = f"{image_url} ****** {alt_description} ****** {image_source}"
                    images.add(formatted_image_link)

            driver.quit()

        except TimeoutException as e:
            print(f"Timeout waiting for image elements on {link}: {e}")
        except Exception as e:
            print(f"Error extracting images using Selenium: {e}")

        return images

    def extract_images_bs4(link, soup):
        """Extracts image links using Beautiful Soup."""
        images = set()
        body = soup.find("body")
        if body is not None:
            body_tags = body.find_all()
            for tag in body_tags:
                if tag.name == "img":
                    if tag.has_attr("src") or tag.has_attr("data-src"):
                        image_url = None
                        image_source = None

                        if tag.has_attr("src"):
                            src = tag["src"].lower()
                            if (
                                    src.endswith(".jpg")
                                    or src.endswith(".jpeg")
                                    or src.endswith(".png")
                                    or re.search(r"\.(jpg|jpeg|png)\?.+", src)
                            ):
                                image_url = urljoin(link, tag["src"])
                                image_source = "src"

                        if tag.has_attr("data-src"):
                            data_src = tag["data-src"].lower()
                            if (
                                    data_src.endswith(".jpg")
                                    or data_src.endswith(".jpeg")
                                    or data_src.endswith(".png")
                                    or re.search(r"\.(jpg|jpeg|png)\?.+", data_src)
                            ):
                                image_url = urljoin(link, tag["data-src"])
                                image_source = "data-src"

                        if image_url is not None:
                            alt_description = tag.get("alt", "NoDescription")
                            formatted_image_link = f"{image_url} ****** {alt_description} ****** {image_source}"
                            images.add(formatted_image_link)

                if tag.name == "a" and tag.has_attr("href"):
                    href = tag["href"].lower()
                    if (
                            href.endswith(".jpg")
                            or href.endswith(".jpeg")
                            or href.endswith(".png")
                            or re.search(r"\.(jpg|jpeg|png)\?.+", href)
                    ):
                        image_url = urljoin(link, tag["href"])
                        alt_description = tag.get("alt", "NoDescription")
                        image_source = "href"
                        formatted_image_link = f"{image_url} ****** {alt_description} ****** {image_source}"
                        images.add(formatted_image_link)
        return images

    def save_data(data, ip_address, layer, data_type, config):
        """Saves extracted data based on the chosen save structure."""
        if config["save_structure"] == "folder":
            save_to_folder(data, ip_address, layer, data_type, config)
        elif config["save_structure"] == "flat":
            save_to_flat(data, ip_address, layer, data_type, config)
        elif config["save_structure"] == "json":
            save_to_json(data, ip_address, layer, data_type, config)
        else:
            print(
                "Invalid save structure. Please choose 'folder', 'flat', or 'json'."
            )

    def save_to_folder(data, ip_address, layer, data_type, config):
        """Saves data to a folder structure."""
        if layer is not None:
            folder_path = os.path.join(config["save_path"], f"Layer_{layer}")
            if data_type == "links":
                filename = f"{data.replace('://', '_').replace('/', '_')}.txt"
                save_path = os.path.join(folder_path, "links", filename)
            elif data_type == "images":
                filename = f"{data.replace('://', '_').replace('/', '_')}.jpg"
                save_path = os.path.join(folder_path, "images", filename)

            os.makedirs(os.path.dirname(save_path), exist_ok=True)
            with open(save_path, "a", encoding="utf-8-sig") as file:
                if data_type == "links":
                    file.write(f"{data}-----{ip_address}\n")
                elif data_type == "images":
                    file.write(f"{data}\n")
        else:
            print("Layer is not provided. Unable to save data to folder.")

    def save_to_flat(data, ip_address, layer, data_type, config):
        """Saves data to a flat file structure."""
        if layer is not None:
            if data_type == "links":
                filename = f"links_layer_{layer}.txt"
                save_path = os.path.join(config["save_path"], filename)
            elif data_type == "images":
                filename = f"images_layer_{layer}.txt"
                save_path = os.path.join(config["save_path"], filename)

            with open(save_path, "a", encoding="utf-8-sig") as file:
                if data_type == "links":
                    file.write(f"{data}-----{ip_address}\n")
                elif data_type == "images":
                    file.write(f"{data}\n")
        else:
            print("Layer is not provided. Unable to save data to flat file.")

    def save_to_json(data, ip_address, layer, data_type, config):
        """Saves data to a JSON file."""
        if layer is not None:
            filename = f"{config['save_path']}.json"
            if os.path.exists(filename):
                with open(filename, "r") as file:
                    try:
                        data_json = json.load(file)
                    except json.JSONDecodeError:
                        print(
                            f"Error: Unable to decode JSON file: {filename}. Proceeding with empty JSON data."
                        )
                        data_json = {}

                if "layers" not in data_json:
                    data_json["layers"] = {}
                if f"Layer_{layer}" not in data_json["layers"]:
                    data_json["layers"][f"Layer_{layer}"] = {}
                if data_type == "links":
                    if "links" not in data_json["layers"][f"Layer_{layer}"]:
                        data_json["layers"][f"Layer_{layer}"]["links"] = []
                    data_json["layers"][f"Layer_{layer}"]["links"].append(
                        {"link": data, "ip": ip_address}
                    )
                elif data_type == "images":
                    if "images" not in data_json["layers"][f"Layer_{layer}"]:
                        data_json["layers"][f"Layer_{layer}"]["images"] = []
                    data_json["layers"][f"Layer_{layer}"]["images"].append(data)

                with open(filename, "w") as file:
                    json.dump(data_json, file, indent=4)
            else:
                with open(filename, "w") as file:
                    data_json = {
                        "layers": {
                            f"Layer_{layer}": {
                                data_type: [
                                    {"link": data, "ip": ip_address}
                                    if data_type == "links"
                                    else data
                                ]
                            }
                        }
                    }
                    json.dump(data_json, file, indent=4)
        else:
            print("Layer is not provided. Unable to save data to JSON file.")

    config = DEFAULT_CONFIG.copy()

    if source is not None:
        config["source"] = source
    if query is not None:
        config["query"] = query
    if initial_processing_method is not None:
        config["initial_processing_method"] = initial_processing_method
    if initial_filtering_phrases is not None:
        config["initial_filtering_phrases"] = initial_filtering_phrases
    if excluded_phrases is not None:
        config["excluded_phrases"] = excluded_phrases
    if image_extraction_method is not None:
        config["image_extraction_method"] = image_extraction_method
    if save_structure is not None:
        config["save_structure"] = save_structure
    if save_path is not None:
        config["save_path"] = save_path
    if max_depth is not None:
        config["max_depth"] = max_depth
    if rate_limit is not None:
        config["rate_limit"] = rate_limit

    initialLinks = get_initial_links(config["source"], config["query"])
    print("Initial links obtained:")
    for link in initialLinks:
        print(link)

    # Process initial links based on the chosen method
    initialLinks = process_initial_set(
        initialLinks, method=config["initial_processing_method"]
    )
    print("\nProcessed initial links:")
    for link in initialLinks:
        print(link)

    # Filter initial links by phrases if provided
    initialLinks = filter_initial_links(
        initialLinks, config["initial_filtering_phrases"]
    )
    print("\nFiltered initial links:")
    for link in initialLinks:
        print(link)

    # Start crawling
    crawl_links(
        initialLinks,
        excluded_phrases=config["excluded_phrases"],
        image_extraction_method=config["image_extraction_method"],
        config=config,
    )

File: TOOL_MANAGER.py (C:\Users\DELL\Desktop\openAIF_frontend\OctopusAI\PROJECT\SMART_BOT4\TOOL_MANAGER.py)
Content (First 158 lines):
## File: TOOL_MANAGER.py (in: C:\Users\DELL\Desktop\selfawareGemini\SelAwareAI_Gemini\AGI_start_4)
import os
import importlib
from typing import Dict, Callable, List, Any
import logging

# Set up logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)


class Tool:
    """Represents a tool that can be used by the AI agent."""

    def __init__(self, name: str, function: Callable, description: str, arguments: Dict[str, str], tool_type: str):
        """
        Initializes a Tool object.

        Args:
            name: The name of the tool.
            function: The callable function that implements the tool.
            description: A brief description of the tool's functionality.
            arguments: A dictionary mapping argument names to their descriptions.
            tool_type: The type of the tool (e.g., 'os', 'web', 'focus').
        """
        self.name = name
        self.function = function
        self.description = description
        self.arguments = arguments
        self.tool_type = tool_type

    def __repr__(self):
        """Returns a string representation of the Tool object."""
        return f"Tool(name='{self.name}', function={self.function.__name__}, description='{self.description}', arguments={self.arguments}, tool_type='{self.tool_type}')"


class ToolManager:
    """Manages and provides access to tools."""

    def __init__(self, tools_folder: str):
        """
        Initializes the ToolManager with the path to the tools folder.

        Args:
            tools_folder: The path to the directory containing tool files.
        """
        self.tools_folder = tools_folder
        self.tools = {}  # Dictionary to store Tool objects
        self.load_tools()

    def load_tools(self):
        """Loads tools from files in the specified tools folder."""
        logger.info(f"Loading tools from: {self.tools_folder}")
        for root, _, files in os.walk(self.tools_folder):
            for file in files:
                if file.endswith(".py"):
                    # Extract tool name from file name
                    tool_name = file[:-3]  # Remove .py extension
                    module_path = os.path.join(root, file)

                    # Import the module
                    try:
                        spec = importlib.util.spec_from_file_location(tool_name, module_path)
                        module = importlib.util.module_from_spec(spec)
                        spec.loader.exec_module(module)
                    except Exception as e:
                        logger.error(f"Error loading tool file '{file}': {e}")
                        continue

                    # Add the tool to the dictionary if it's a function
                    for attr_name in dir(module):
                        attr = getattr(module, attr_name)
                        if callable(attr):
                            # Get the tool name from the function name
                            tool_name = attr_name

                            # Construct the tool path for the main loop to use
                            relative_path = os.path.relpath(module_path, self.tools_folder)

                            # Define tool descriptions and arguments (you might want to customize these)
                            tool_description = f"Tool for {tool_name}"
                            tool_arguments = {
                                'file_path': 'The path to the file',
                                'content': 'The content to be saved',
                                # Add more arguments as needed for specific tools
                            }

                            # Get the tool type from the file (assuming it's a variable named 'tool_type_for_TOOL_MANAGER')
                            tool_type = getattr(module, 'tool_type_for_TOOL_MANAGER', 'unknown')

                            # Store Tool object for better information
                            self.tools[tool_name] = Tool(tool_name, attr, tool_description, tool_arguments, tool_type)

                            logger.info(f"Discovered tool: {tool_name} (Type: {tool_type})")
                            print(f"  - {tool_name} - {tool_description}")  # Add a nice print statement
                            logger.debug(f"Tool description: {tool_description}")
                            logger.debug(f"Tool arguments: {tool_arguments}")

    def get_tool_function(self, function_name: str) -> Callable:
        """Returns the callable object for the given function name."""
        tool = self.tools.get(function_name)
        if tool:
            return tool.function
        else:
            return None

    def get_all_tools(self) -> List[Tool]:
        """Returns a list of all loaded tools."""
        return list(self.tools.values())

    def get_tools_by_type(self, tool_type: str) -> List[Tool]:
        """Returns a list of tools based on their type."""
        return [tool for tool in self.tools.values() if tool.tool_type == tool_type]

    def load_tools_of_type(self, tool_type: str = "all") -> List[Callable]:
        """Loads and returns a list of tool functions based on the specified type.

        Args:
            tool_type: The type of tools to load. 'all' for all tools, or a specific type like 'os', 'web', etc.

        Returns:
            A list of tool functions.
        """
        if tool_type == "all":
            return [tool.function for tool in self.tools.values()]
        else:
            return [tool.function for tool in self.tools.values() if tool.tool_type == tool_type]

    def call_tool(self, tool_name: str, arguments: Dict[str, Any]) -> Any:
        """
        Calls the tool function with the provided arguments.

        Args:
            tool_name: The name of the tool to call.
            arguments: A dictionary of arguments to pass to the tool function.

        Returns:
            The result of the tool function call.

        Raises:
            KeyError: If the tool name is not found.
            TypeError: If the provided arguments are not valid for the tool.
        """
        tool = self.tools.get(tool_name)
        if tool is None:
            raise KeyError(f"Tool '{tool_name}' not found.")

        # Check if all required arguments are provided
        missing_args = set(tool.arguments.keys()) - set(arguments.keys())
        if missing_args:
            raise TypeError(f"Missing arguments for tool '{tool_name}': {', '.join(missing_args)}")

        # Call the tool function
        try:
            result = tool.function(**arguments)
            return result
        except Exception as e:
            raise RuntimeError(f"Error calling tool '{tool_name}': {e}")


Subdirectory: __pycache__
## Summary of Files and Directories in 'C:\Users\DELL\Desktop\openAIF_frontend\OctopusAI\PROJECT\SMART_BOT4\__pycache__'

File: keys.cpython-312.pyc (C:\Users\DELL\Desktop\openAIF_frontend\OctopusAI\PROJECT\SMART_BOT4\__pycache__\keys.cpython-312.pyc)
Error decoding file 'C:\Users\DELL\Desktop\openAIF_frontend\OctopusAI\PROJECT\SMART_BOT4\__pycache__\keys.cpython-312.pyc': 'utf-8' codec can't decode byte 0xcb in position 0: invalid continuation byte

File: TOOL_MANAGER.cpython-312.pyc (C:\Users\DELL\Desktop\openAIF_frontend\OctopusAI\PROJECT\SMART_BOT4\__pycache__\TOOL_MANAGER.cpython-312.pyc)
Error decoding file 'C:\Users\DELL\Desktop\openAIF_frontend\OctopusAI\PROJECT\SMART_BOT4\__pycache__\TOOL_MANAGER.cpython-312.pyc': 'utf-8' codec can't decode byte 0xcb in position 0: invalid continuation byte

