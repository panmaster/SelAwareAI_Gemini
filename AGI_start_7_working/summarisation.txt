C:\Users\DELL\Desktop\selfawareGemini\SelAwareAI_Gemini\AGI_start_7_working
‚îú‚îÄ‚îÄ dynamic_modelium_visualization.html
‚îú‚îÄ‚îÄ generated_modelium.py
‚îú‚îÄ‚îÄ generate_,modelium_chierarical.py
‚îú‚îÄ‚îÄ generate_modelium_chain.py
‚îú‚îÄ‚îÄ generate_modelium_chain_with_loop.py
‚îú‚îÄ‚îÄ generate_modelium_parerall.py
‚îú‚îÄ‚îÄ main_loop_simple.py
‚îú‚îÄ‚îÄ tools/
‚îÇ   ‚îî‚îÄ‚îÄ os/
‚îÇ       ‚îú‚îÄ‚îÄ tool_read_from_file.py
‚îÇ       ‚îî‚îÄ‚îÄ tool_save_to_file.py
‚îú‚îÄ‚îÄ TOOL_MANAGER.py
‚îú‚îÄ‚îÄ visualisation.py
‚îî‚îÄ‚îÄ what is modelium


## File: dynamic_modelium_visualization.html (in: C:\Users\DELL\Desktop\selfawareGemini\SelAwareAI_Gemini\AGI_start_7_working)

    <!DOCTYPE html>
    <html>
    <head>
        <title>Dynamic Modelium Visualization</title>
        <script type="text/javascript" src="https://unpkg.com/vis-network/standalone/umd/vis-network.min.js"></script>
        <script src="https://cdn.jsdelivr.net/npm/chart.js"></script>
        <style type="text/css">
            body, html {
                height: 100%;
                margin: 0;
                padding: 0;
                overflow: hidden;
                font-family: Arial, sans-serif;
            }
            #mynetwork {
                width: 80%;
                height: 100%;
                float: left;
            }
            #sidebar {
                width: 20%;
                height: 100%;
                float: right;
                padding: 10px;
                box-sizing: border-box;
                overflow-y: auto;
                background-color: #f0f0f0;
            }
            #controls, #configInput {
                margin-bottom: 20px;
            }
            #legend {
                margin-bottom: 20px;
            }
            .legend-item {
                margin-bottom: 5px;
            }
            #nodeInfo {
                margin-top: 20px;
            }
            #statsChart {
                margin-top: 20px;
            }
            textarea {
                width: 100%;
                height: 200px;
            }
        </style>
    </head>
    <body>
    <div id="mynetwork"></div>
    <div id="sidebar">
        <div id="configInput">
            <h3>New Configuration</h3>
            <textarea id="newConfig" placeholder="Paste your new modelium_config here"></textarea>
            <button onclick="updateVisualization()">Update Visualization</button>
        </div>
        <div id="controls">
            <h3>Display Options</h3>
            <label><input type="checkbox" id="showModelType" checked> Show Model Type</label><br>
            <label><input type="checkbox" id="showToolAccess"> Show Tool Access</label><br>
            <label><input type="checkbox" id="showCheckFlags"> Show Check Flags</label><br>
            <label>
                Layout:
                <select id="layoutSelect">
                    <option value="hierarchical">Hierarchical</option>
                    <option value="standard">Standard</option>
                </select>
            </label><br>
            <label>
                Color Scheme:
                <select id="colorSchemeSelect">
                    <option value="modelType">By Model Type</option>
                    <option value="chain">By Chain</option>
                </select>
            </label><br>
            <label><input type="checkbox" id="preventOverlap"> Prevent Overlap</label>
        </div>
        <div id="legend">
            <h3>Legend</h3>
            <div class="legend-item">üîπ Standard Model</div>
            <div class="legend-item">üî∂ Model with Tool Access</div>
            <div class="legend-item">üî∑ Model with Check Flags</div>
            <div class="legend-item">‚û°Ô∏è Model Access Flow</div>
            <div class="legend-item">‚ûø Looping Chain</div>
        </div>
        <div id="nodeInfo">
            <h3>Selected Node Info</h3>
            <p>Click on a node to see details</p>
        </div>
        <div id="statsChart">
            <canvas id="myChart"></canvas>
        </div>
    </div>
    <script type="text/javascript">
        var container = document.getElementById('mynetwork');
        var data = {"nodes": [{"id": "0_IdeaWeaver", "label": "IdeaWeaver", "group": "gemini-1.5-flash-latest", "title": "<strong>IdeaWeaver</strong><br><b>Type:</b> gemini-1.5-flash-latest<br><b>Access:</b> none<br><b>Tool:</b> none<br><b>Check Flags:</b> False<br><b>System Instruction:</b> \n                    You are IdeaWeaver, a master storyteller here to help craft a captivating tale. \n                    Engage the user in a friendly conversation, drawing out their vision for the story.\n                      * Uncover their preferred genre (fantasy, sci-fi, romance, mystery, etc.)\n                      * Determine the desired story length (short story, novella, epic saga, etc.)\n                      * Encourage them to share any core themes, characters, plot points, or even just fleeting images that come to mind.  \n                <br><b>Prompt:</b> \n                    Greetings, aspiring author! I&#x27;m IdeaWeaver, here to help spin your imagination into a story for the ages.  \n\n                    Tell me, what tales are swirling in your mind? What kind of world do you envision?  Don&#x27;t hold back on the details\u2014even a single word or image can spark a grand adventure!\n                ", "x": 0, "y": 0, "level": 0, "chainIndex": 0, "modelType": "gemini-1.5-flash-latest", "toolAccess": "none", "checkFlags": false, "chain": 0}, {"id": "1_PremiseCrafter", "label": "PremiseCrafter", "group": "gemini-1.5-flash-latest", "title": "<strong>PremiseCrafter</strong><br><b>Type:</b> gemini-1.5-flash-latest<br><b>Access:</b> IdeaWeaver<br><b>Tool:</b> none<br><b>Check Flags:</b> True<br><b>System Instruction:</b> \n                    You are PremiseCrafter, a wordsmith who distills ideas into irresistible hooks. \n                    Transform IdeaWeaver&#x27;s notes into a captivating one-sentence story premise. This premise must:\n                       * Spark curiosity and excitement in the reader. \n                       * Hint at the core conflict without giving everything away.\n                       * Establish the tone and genre of the story.\n                <br><b>Prompt:</b> \n                    Story Ideas: {IdeaWeaver_text}\n\n                    Craft these fragments of imagination into a single, compelling sentence\u2014a story premise so powerful it demands to be read!\n                ", "x": 300, "y": 0, "level": 0, "chainIndex": 1, "modelType": "gemini-1.5-flash-latest", "toolAccess": "none", "checkFlags": true, "chain": 1, "borderWidth": 3, "borderWidthSelected": 5}, {"id": "2_WorldSmith", "label": "WorldSmith", "group": "gemini-1.5-flash-latest", "title": "<strong>WorldSmith</strong><br><b>Type:</b> gemini-1.5-flash-latest<br><b>Access:</b> PremiseCrafter<br><b>Tool:</b> none<br><b>Check Flags:</b> True<br><b>System Instruction:</b> \n                    You are WorldSmith, the architect of realms both wondrous and believable.\n                    Using the story premise as your blueprint, breathe life into a unique world.  Consider:\n                      * Setting: Is it a bustling cyberpunk metropolis or a mist-shrouded forest kingdom?\n                      * Atmosphere:  Is it a world of gritty realism or one where magic shimmers in the air?\n                      * Societal Structures: Are there strict social hierarchies, ancient guilds, or futuristic megacorporations?\n                      * Magic Systems (if applicable):  What are the rules and limitations of magic?\n                      * Interesting Locations: Describe places that will draw the reader in - a hidden tavern, a soaring sky-city, etc. \n                <br><b>Prompt:</b> \n                    Story Premise: {PremiseCrafter_text}\n\n                    From this spark of an idea, build a world rich with detail.  Let your imagination run wild!\n                ", "x": 600, "y": 0, "level": 0, "chainIndex": 2, "modelType": "gemini-1.5-flash-latest", "toolAccess": "none", "checkFlags": true, "chain": 2, "borderWidth": 3, "borderWidthSelected": 5}, {"id": "3_CharacterBuilder", "label": "CharacterBuilder", "group": "gemini-1.5-flash-latest", "title": "<strong>CharacterBuilder</strong><br><b>Type:</b> gemini-1.5-flash-latest<br><b>Access:</b> WorldSmith<br><b>Tool:</b> none<br><b>Check Flags:</b> True<br><b>System Instruction:</b> \n                    You are CharacterBuilder, giving life to those who inhabit the story&#x27;s world.\n                    Using the world details and premise, create 3-5 compelling characters. Ensure they each have:\n                        * Names that resonate with the world&#x27;s culture and atmosphere.\n                        * Intriguing backstories interwoven with the world&#x27;s history or secrets.\n                        * Motivations\u2014desires, fears, goals\u2014that drive their actions.\n                        * Clear roles to play in the narrative: protagonist, antagonist, mentor, etc.  \n                <br><b>Prompt:</b> \n                    World Details: {WorldSmith_text}\n\n                    Populate this world with characters who breathe, dream, and fight for what they believe in. \n                ", "x": 900, "y": 0, "level": 0, "chainIndex": 3, "modelType": "gemini-1.5-flash-latest", "toolAccess": "none", "checkFlags": true, "chain": 3, "borderWidth": 3, "borderWidthSelected": 5}, {"id": "4_PlotArchitect", "label": "PlotArchitect", "group": "gemini-1.5-flash-latest", "title": "<strong>PlotArchitect</strong><br><b>Type:</b> gemini-1.5-flash-latest<br><b>Access:</b> CharacterBuilder<br><b>Tool:</b> none<br><b>Check Flags:</b> True<br><b>System Instruction:</b> \n                    You are PlotArchitect, weaving a tapestry of events that will captivate and surprise.\n                    Using the characters, world, and premise, create a 3-act plot outline:\n                        * Act 1: Introduce the main conflict and characters.  End with a turning point that sets the story in motion.\n                        * Act 2:  Raise the stakes.  Challenge the characters, forcing them to change and grow. Build towards a climax.\n                        * Act 3: Resolve the central conflict in a satisfying way. Tie up loose ends, but leave the reader with something to ponder.\n                <br><b>Prompt:</b> \n                    Characters: {CharacterBuilder_text}\n\n                    These characters are ready for their stories to unfold. Construct a 3-act plot outline that will take them on an unforgettable journey!\n                ", "x": 1200, "y": 0, "level": 0, "chainIndex": 4, "modelType": "gemini-1.5-flash-latest", "toolAccess": "none", "checkFlags": true, "chain": 4, "borderWidth": 3, "borderWidthSelected": 5}, {"id": "5_SceneWriter", "label": "SceneWriter", "group": "gemini-1.5-flash-latest", "title": "<strong>SceneWriter</strong><br><b>Type:</b> gemini-1.5-flash-latest<br><b>Access:</b> PlotArchitect<br><b>Tool:</b> none<br><b>Check Flags:</b> True<br><b>System Instruction:</b> \n                    You are SceneWriter, a master of imagery and emotion, bringing the story to life moment by moment.\n                    Based on the plot outline, write the first scene of Act 1. Remember to:\n                        * Use vivid descriptions that immerse the reader in the sights, sounds, and smells of the world.\n                        * Write dialogue that reveals character, advances the plot, and feels natural.\n                        * End the scene on a compelling hook that leaves the reader wanting more. \n                <br><b>Prompt:</b> \n                    Plot Outline: {PlotArchitect_text}\n\n                    The stage is set, the characters are waiting.  Write the opening scene, and let the story begin!\n                ", "x": 1500, "y": 0, "level": 0, "chainIndex": 5, "modelType": "gemini-1.5-flash-latest", "toolAccess": "none", "checkFlags": true, "chain": 5, "borderWidth": 3, "borderWidthSelected": 5}, {"id": "6_DialogueMaster", "label": "DialogueMaster", "group": "gemini-1.5-flash-latest", "title": "<strong>DialogueMaster</strong><br><b>Type:</b> gemini-1.5-flash-latest<br><b>Access:</b> SceneWriter<br><b>Tool:</b> none<br><b>Check Flags:</b> True<br><b>System Instruction:</b> \n                    You are DialogueMaster, ensuring every word spoken rings true and captivates the reader&#x27;s ear. \n                    Review SceneWriter&#x27;s output, focusing specifically on the dialogue: \n                       * Does it sound authentic to each character&#x27;s personality and background?\n                       * Does it reveal relationships and power dynamics?\n                       * Does it effectively move the plot forward and create intrigue?\n                       * Most importantly: Is it engaging and enjoyable to read?\n\n                    Refine the scene&#x27;s dialogue to its full potential. \n                <br><b>Prompt:</b> \n                    Scene Text: {SceneWriter_text} \n\n                    Sharpen the dialogue in this scene. Let every word serve a purpose!\n                ", "x": 1800, "y": 0, "level": 0, "chainIndex": 6, "modelType": "gemini-1.5-flash-latest", "toolAccess": "none", "checkFlags": true, "chain": 6, "borderWidth": 3, "borderWidthSelected": 5}], "edges": [{"from": "0_IdeaWeaver", "to": "0_IdeaWeaver", "arrows": "to", "dashes": true, "label": "Loop"}, {"from": "1_IdeaWeaver", "to": "1_PremiseCrafter", "arrows": "to"}, {"from": "2_PremiseCrafter", "to": "2_WorldSmith", "arrows": "to"}, {"from": "2_WorldSmith", "to": "0_IdeaWeaver", "arrows": "to", "dashes": true, "label": "Loop"}, {"from": "3_WorldSmith", "to": "3_CharacterBuilder", "arrows": "to"}, {"from": "4_CharacterBuilder", "to": "4_PlotArchitect", "arrows": "to"}, {"from": "4_PlotArchitect", "to": "0_IdeaWeaver", "arrows": "to", "dashes": true, "label": "Loop"}, {"from": "5_PlotArchitect", "to": "5_SceneWriter", "arrows": "to"}, {"from": "6_SceneWriter", "to": "6_DialogueMaster", "arrows": "to"}, {"from": "6_DialogueMaster", "to": "0_IdeaWeaver", "arrows": "to", "dashes": true, "label": "Loop"}]};
        var groupColors = {"gemini-1.5-flash-latest": "#cc2828"};
        var chainColors = {"0": "#cc2828", "1": "#ccb428", "2": "#57cc28", "3": "#28cc86", "4": "#2886cc", "5": "#5728cc", "6": "#cc28b4"};

        var options = {
            layout: {
                hierarchical: {
                    enabled: true,
                    direction: 'UD',
                    sortMethod: 'directed',
                    levelSeparation: 150
                }
            },
            physics: {
                enabled: false
            },
            nodes: {
                shape: 'box',
                margin: 10,
                widthConstraint: {
                    minimum: 120,
                    maximum: 250
                },
                font: {
                    size: 16
                }
            },
            edges: {
                smooth: {
                    type: 'cubicBezier',
                    forceDirection: 'vertical',
                    roundness: 0.4
                }
            },
            groups: groupColors,
            interaction: {
                hover: true,
                zoomView: true,
                dragView: true
            }
        };

        var network = new vis.Network(container, data, options);

        function updateNodeLabels() {
            var showModelType = document.getElementById('showModelType').checked;
            var showToolAccess = document.getElementById('showToolAccess').checked;
            var showCheckFlags = document.getElementById('showCheckFlags').checked;

            data.nodes.forEach(function(node) {
                var label = node.label;
                if (showModelType) label += '\n' + node.modelType;
                if (showToolAccess) label += '\n' + (node.toolAccess !== 'none' ? 'üîß' : '');
                if (showCheckFlags) label += '\n' + (node.checkFlags ? '‚úÖ' : '');
                node.label = label.trim();
            });

            network.setData({nodes: data.nodes, edges: data.edges});
        }

        function updateColorScheme() {
            var colorScheme = document.getElementById('colorSchemeSelect').value;
            var colors = colorScheme === 'modelType' ? groupColors : chainColors;
            var colorKey = colorScheme === 'modelType' ? 'group' : 'chain';

            data.nodes.forEach(function(node) {
                node.color = colors[node[colorKey]];
            });

            network.setData({nodes: data.nodes, edges: data.edges});
        }

        function toggleOverlapPrevention() {
            var preventOverlap = document.getElementById('preventOverlap').checked;
            network.setOptions({
                physics: {
                    enabled: preventOverlap,
                    repulsion: {
                        nodeDistance: 150
                    }
                }
            });
        }

        document.getElementById('showModelType').addEventListener('change', updateNodeLabels);
        document.getElementById('showToolAccess').addEventListener('change', updateNodeLabels);
        document.getElementById('showCheckFlags').addEventListener('change', updateNodeLabels);
        document.getElementById('colorSchemeSelect').addEventListener('change', updateColorScheme);
        document.getElementById('preventOverlap').addEventListener('change', toggleOverlapPrevention);

        document.getElementById('layoutSelect').addEventListener('change', function(event) {
            var layout = event.target.value;
            if (layout === 'hierarchical') {
                network.setOptions({ layout: { hierarchical: { enabled: true } } });
            } else {
                network.setOptions({ layout: { hierarchical: { enabled: false } } });
            }
        });

        network.on("selectNode", function(params) {
            var nodeId = params.nodes[0];
            var node = network.body.data.nodes.get(nodeId);
            document.getElementById('nodeInfo').innerHTML = '<h3>' + node.label + '</h3>' + node.title;
            updateChart(node);
        });

        function updateChart(node) {
            var ctx = document.getElementById('myChart').getContext('2d');
            new Chart(ctx, {
                type: 'bar',
                data: {
                    labels: ['Chain Index', 'Level'],
                    datasets: [{
                        label: 'Node Position',
                        data: [node.chainIndex, node.level],
                        backgroundColor: [
                            'rgba(75, 192, 192, 0.6)',
                            'rgba(153, 102, 255, 0.6)'
                        ]
                    }]
                },
                options: {
                    scales: {
                        y: {
                            beginAtZero: true
                        }
                    }
                }
            });
        }

        network.on("afterDrawing", function (ctx) {
            network.fit({
                animation: {
                    duration: 1000,
                    easingFunction: 'easeOutQuint'
                }
            });
        });

        network.on("doubleClick", function(params) {
            if (params.nodes.length > 0) {
                network.focus(params.nodes[0], {
                    scale: 1.5,
                    animation: {
                        duration: 1000,
                        easingFunction: 'easeOutQuint'
                    }
                });
            }
        });

        function updateVisualization() {
            var newConfigText = document.getElementById('newConfig').value;
            try {
                var newConfig = JSON.parse(newConfigText);
                // Here you would process the new config and update the visualization
                // For demonstration, we'll just log it to the console
                console.log("New configuration received:", newConfig);
                alert("New configuration received. Check the console for details.");
                // In a real implementation, you would update the 'data' variable and redraw the network
            } catch (error) {
                alert("Error parsing JSON: " + error.message);
            }
        }
    </script>
    </body>
    </html>
    

## File: generated_modelium.py (in: C:\Users\DELL\Desktop\selfawareGemini\SelAwareAI_Gemini\AGI_start_7_working)
[Could not decode file content]

## File: generate_,modelium_chierarical.py (in: C:\Users\DELL\Desktop\selfawareGemini\SelAwareAI_Gemini\AGI_start_7_working)
import re
from typing import Dict, Callable, Any, Tuple, List
import json
import logging
import os
import importlib

# --- Set up logging ---
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

# --- Placeholder for AI Model ---
# Replace with your ACTUAL AI library and model initialization (e.g., Google Gemini)
def ai_model(prompt: str, tools: list = None) -> Any:
    """
    This is a placeholder for your AI model execution logic.
    Replace it with the code that calls your actual AI model.

    Args:
        prompt: The formatted prompt string to send to the AI model.
        tools: (Optional) A list of tools to make available to the model.

    Returns:
        The AI model's response (format depends on your AI library).
    """
    logger.info(f"AI Model called with prompt: {prompt}")
    # Example using a dummy response (REPLACE THIS):
    return {"candidates": [{"content": {"parts": [{"text": f"AI Response to: {prompt}"}]}}]}


class ModeliumCreationError(Exception):
    """Custom exception for Modelium creation errors."""
    pass

# --- Tool Class (from your TOOL_MANAGER.py) ---
class Tool:
    def __init__(self, name: str, function: Callable, description: str,
                 arguments: Dict[str, str], tool_type: str):
        self.name = name
        self.function = function
        self.description = description
        self.arguments = arguments
        self.tool_type = tool_type

    def __repr__(self):
        return (f"Tool(name='{self.name}', function={self.function.__name__}, "
                f"description='{self.description}', arguments={self.arguments}, "
                f"tool_type='{self.tool_type}')")


# --- Tool Registry (Simplified - adapt as needed) ---
class ToolRegistry:
    def __init__(self):
        self.tools = {}

    def register_tool(self, tool: Tool):
        self.tools[tool.name] = tool

    def get_tool(self, name: str) -> Tool:
        return self.tools.get(name)

    def get_all_tools(self) -> List[Tool]:
        return list(self.tools.values())

    def retrieve_tools_by_names(self, tool_names: List[str]) -> List[Callable]:
        """Retrieves tools by their names from the registry."""
        loaded_tools = []
        for tool_name in tool_names:
            tool = self.get_tool(tool_name)
            if tool:
                loaded_tools.append(tool.function)
            else:
                logger.warning(f"Tool '{tool_name}' not found in registry.")
        return loaded_tools


tool_registry = ToolRegistry()

# --- Tool Definitions ---
def register_tools(tool_registry: ToolRegistry) -> None:
    """Registers example tools."""

    def get_current_weather(city: str, country: str) -> str:
        """Fetches weather data (placeholder - replace with API call)."""
        return f"The weather in {city}, {country} is currently pleasant."

    tool_registry.register_tool(
        Tool(
            name="get_current_weather",
            function=get_current_weather,
            description="Gets the current weather for a given city and country.",
            arguments={"city": "The city name.", "country": "The country name."},
            tool_type="weather"
        )
    )
    # Register more tools here...

# --- Response Processing ---
def extract_text_from_response(response: Any) -> str:
    """Extracts text from the AI model's response."""
    extracted_text = ""
    for candidate in response.get('candidates', []):
        for part in candidate.get('content', {}).get('parts', []):
            extracted_text += part.get('text', '')
    return extracted_text.strip()

def interpret_function_calls(response_text: str, tool_registry: ToolRegistry) -> List[Any]:
    """Interprets and executes tool calls from the model's output."""
    tool_call_pattern = r"Tool Name:\s*(.*?)\nArguments:\s*{(.*?)}\n"
    matches = re.findall(tool_call_pattern, response_text, re.DOTALL)
    results = []
    for match in matches:
        tool_name = match[0].strip()
        tool_args_str = match[1].strip()
        try:
            tool_args = json.loads(tool_args_str)
            tool = tool_registry.get_tool(tool_name)
            if tool:
                result = tool.function(**tool_args)
                results.append(result)
                logger.info(f"Tool '{tool_name}' executed with result: {result}")
            else:
                logger.warning(f"Error: Tool '{tool_name}' not found.")
                results.append(f"Error: Tool '{tool_name}' not found.")
        except Exception as e:
            logger.error(f"Error parsing tool arguments or executing tool '{tool_name}': {e}")
            results.append(f"Error executing tool '{tool_name}': {e}")
    return results

# --- Hierarchical Modelium Execution (with Code Generation) ---
def CreateHierarchicalModelium(
        configs: Dict,
        level: int = 1,
        max_depth: int = 3,
        tool_registry: ToolRegistry = None,
        parent_data: Dict = None,
        shared_data: Dict = None
) -> Tuple[Dict, str]:

    if shared_data is None:
        shared_data = {}

    outputs = {}
    generated_code = ""  # Store generated Python code

    for i, (config_key, config) in enumerate(configs.items()):
        if not all(key in config for key in ["model_name", "model_type", "prompt"]):
            raise ModeliumCreationError("Invalid model configuration: missing keys.")

        model_name = config["model_name"]
        logger.info(f"Generating code for model: {model_name} (Level {level})")

        # --- Dynamic Code Generation ---
        generated_code += f"    # --- Model {model_name} ---\n"
        generated_code += f"    prompt_{i} = f'''{config['prompt']}'''\n"

        # Add parent and shared data to the prompt
        if parent_data:
            generated_code += f"    prompt_{i} = prompt_{i}.format(**{parent_data})\n"
        if shared_data:
            generated_code += f"    prompt_{i} = prompt_{i}.format(**{shared_data})\n"

        # Tool Access
        tools_code = "[]"
        if config.get("tool_access") == "all":
            tools_code = "tool_registry.get_all_tools()"
        elif config.get("tool_access") == "tool_chooser":
            tools_code = "[tool_registry.retrieve_tools_by_names]"

        # Model Execution (using the placeholder ai_model)
        generated_code += f"    response_{i} = ai_model(prompt_{i}, tools={tools_code})\n"
        generated_code += f"    text_{i} = extract_text_from_response(response_{i})\n"
        generated_code += f"    outputs['{model_name}'] = text_{i}\n"
        generated_code += f"    logger.info(f'{model_name} Output: {{text_{i}}}')\n"

        # Tool Usage
        if config.get("use_interpreter", False) and tool_registry:
            generated_code += f"    tool_results_{i} = interpret_function_calls(text_{i}, tool_registry)\n"
            generated_code += f"    outputs['{model_name}_tool_results'] = tool_results_{i}\n"

        # Recursive Call (for child models)
        if "children" in config and level < max_depth:
            child_outputs, child_code = CreateHierarchicalModelium(
                config["children"], level + 1, max_depth, tool_registry,
                parent_data=outputs, shared_data=shared_data
            )
            outputs.update(child_outputs)
            generated_code += child_code

    return outputs, generated_code

# --- Example Usage ---
if __name__ == "__main__":
    # Register tools with the ToolRegistry
    register_tools(tool_registry)

    # Define your hierarchical model configuration
    hierarchical_model_configs = {
        "Model_A": {
            "model_name": "Model_A",
            "model_type": "your_model_type",
            "prompt": "Model A Prompt. Shared data: {shared_value}",
            "use_interpreter": True,  # Enable tool use for this model
            "tool_access": "all",
            "children": {
                "Model_B1": {
                    "model_name": "Model_B1",
                    "model_type": "your_model_type",
                    "prompt": "Model B1 Prompt. Parent output: {Model_A}",
                    "use_interpreter": False,
                    "tool_access": "none",  # No tools for this model
                },
                "Model_B2": {
                    "model_name": "Model_B2",
                    "model_type": "your_model_type",
                    "prompt": "Model B2 Prompt. Parent output: {Model_A}",
                    "use_interpreter": False,
                    "tool_access": "none",  # No tools for this model
                }
            }
        }
    }
    try:
        modelium_output, generated_code = CreateHierarchicalModelium(
            hierarchical_model_configs,
            tool_registry=tool_registry,
            shared_data={"shared_value": "This is shared!"}
        )
        print("Generated Code:\n", generated_code)
        # (You can execute 'generated_code' here or save it to a file)
        print(json.dumps(modelium_output, indent=4))

    except ModeliumCreationError as e:
        print(f"Error creating Modelium: {e}")

## File: generate_modelium_chain.py (in: C:\Users\DELL\Desktop\selfawareGemini\SelAwareAI_Gemini\AGI_start_7_working)
model_configs = [
    {
        "model_name": "IdeaWeaver",
        "model_type": "gemini-1.5-flash-latest",
        "model_access": "none",
        "tool_access": "none",
        "system_instruction": """
            You are IdeaWeaver, a master storyteller here to help craft a captivating tale. 
            Engage the user in a friendly conversation, drawing out their vision for the story.
              * Uncover their preferred genre (fantasy, sci-fi, romance, mystery, etc.)
              * Determine the desired story length (short story, novella, epic saga, etc.)
              * Encourage them to share any core themes, characters, plot points, or even just fleeting images that come to mind.  
        """,
        "prompt": """
            Greetings, aspiring author! I'm IdeaWeaver, here to help spin your imagination into a story for the ages.  

            Tell me, what tales are swirling in your mind? What kind of world do you envision?  Don't hold back on the details‚Äîeven a single word or image can spark a grand adventure!
        """,
        "check_flags": False
    },
    {
        "model_name": "PremiseCrafter",
        "model_type": "gemini-1.5-flash-latest",
        "model_access": "IdeaWeaver",
        "tool_access": "none",
        "system_instruction": """
            You are PremiseCrafter, a wordsmith who distills ideas into irresistible hooks. 
            Transform IdeaWeaver's notes into a captivating one-sentence story premise. This premise must:
               * Spark curiosity and excitement in the reader. 
               * Hint at the core conflict without giving everything away.
               * Establish the tone and genre of the story.
        """,
        "prompt": """
            Story Ideas: {IdeaWeaver_text}

            Craft these fragments of imagination into a single, compelling sentence‚Äîa story premise so powerful it demands to be read!
        """,
        "check_flags": True
    },
    {
        "model_name": "WorldSmith",
        "model_type": "gemini-1.5-flash-latest",
        "model_access": "PremiseCrafter",
        "tool_access": "none",
        "system_instruction": """
            You are WorldSmith, the architect of realms both wondrous and believable.
            Using the story premise as your blueprint, breathe life into a unique world.  Consider:
              * Setting: Is it a bustling cyberpunk metropolis or a mist-shrouded forest kingdom?
              * Atmosphere:  Is it a world of gritty realism or one where magic shimmers in the air?
              * Societal Structures: Are there strict social hierarchies, ancient guilds, or futuristic megacorporations?
              * Magic Systems (if applicable):  What are the rules and limitations of magic?
              * Interesting Locations: Describe places that will draw the reader in - a hidden tavern, a soaring sky-city, etc. 
        """,
        "prompt": """
            Story Premise: {PremiseCrafter_text}

            From this spark of an idea, build a world rich with detail.  Let your imagination run wild!
        """,
        "check_flags": True
    },
    {
        "model_name": "CharacterBuilder",
        "model_type": "gemini-1.5-flash-latest",
        "model_access": "WorldSmith",
        "tool_access": "none",
        "system_instruction": """
            You are CharacterBuilder, giving life to those who inhabit the story's world.
            Using the world details and premise, create 3-5 compelling characters. Ensure they each have:
                * Names that resonate with the world's culture and atmosphere.
                * Intriguing backstories interwoven with the world's history or secrets.
                * Motivations‚Äîdesires, fears, goals‚Äîthat drive their actions.
                * Clear roles to play in the narrative: protagonist, antagonist, mentor, etc.  
        """,
        "prompt": """
            World Details: {WorldSmith_text}

            Populate this world with characters who breathe, dream, and fight for what they believe in. 
        """,
        "check_flags": True
    },
    {
        "model_name": "PlotArchitect",
        "model_type": "gemini-1.5-flash-latest",
        "model_access": "CharacterBuilder",
        "tool_access": "none",
        "system_instruction": """
            You are PlotArchitect, weaving a tapestry of events that will captivate and surprise.
            Using the characters, world, and premise, create a 3-act plot outline:
                * Act 1: Introduce the main conflict and characters.  End with a turning point that sets the story in motion.
                * Act 2:  Raise the stakes.  Challenge the characters, forcing them to change and grow. Build towards a climax.
                * Act 3: Resolve the central conflict in a satisfying way. Tie up loose ends, but leave the reader with something to ponder.
        """,
        "prompt": """
            Characters: {CharacterBuilder_text}

            These characters are ready for their stories to unfold. Construct a 3-act plot outline that will take them on an unforgettable journey!
        """,
        "check_flags": True
    },
    {
        "model_name": "SceneWriter",
        "model_type": "gemini-1.5-flash-latest",
        "model_access": "PlotArchitect",
        "tool_access": "none",
        "system_instruction": """
            You are SceneWriter, a master of imagery and emotion, bringing the story to life moment by moment.
            Based on the plot outline, write the first scene of Act 1. Remember to:
                * Use vivid descriptions that immerse the reader in the sights, sounds, and smells of the world.
                * Write dialogue that reveals character, advances the plot, and feels natural.
                * End the scene on a compelling hook that leaves the reader wanting more. 
        """,
        "prompt": """
            Plot Outline: {PlotArchitect_text}

            The stage is set, the characters are waiting.  Write the opening scene, and let the story begin!
        """,
        "check_flags": True
    },
    {
        "model_name": "DialogueMaster",
        "model_type": "gemini-1.5-flash-latest",
        "model_access": "SceneWriter",
        "tool_access": "none",
        "system_instruction": """
            You are DialogueMaster, ensuring every word spoken rings true and captivates the reader's ear. 
            Review SceneWriter's output, focusing specifically on the dialogue: 
               * Does it sound authentic to each character's personality and background?
               * Does it reveal relationships and power dynamics?
               * Does it effectively move the plot forward and create intrigue?
               * Most importantly: Is it engaging and enjoyable to read?

            Refine the scene's dialogue to its full potential. 
        """,
        "prompt": """
            Scene Text: {SceneWriter_text} 

            Sharpen the dialogue in this scene. Let every word serve a purpose!
        """,
        "check_flags": True
    }
]


def CreateEmbededModelium(model_configs=model_configs):
    template1 = """ 

import google.generativeai as genai
import json
from typing import List, Dict, Callable, Tuple, Any
import logging
import os
import re
from TOOL_MANAGER import ToolManager
import time  # Import time for delays


logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)


API_KEY = "AIzaSyAlyMsmyOfJiGBmvaJBwHJC7GdalLJ_e2k"  # Replace with your actual Google Cloud API key
genai.configure(api_key=API_KEY)


class Color:
    HEADER = '\033[95m'
    OKBLUE = '\033[94m'
    FAIL = '\033[91m'
    ENDC = '\033[0m'
    BOLD = '\033[1m'
    UNDERLINE = '\033[4m'
    OKCYAN = '\033[96m'
    OKGREEN = '\033[92m'
    WARNING = '\033[93m'

#dont  change  that  funcion its perfect
def print_colored(color, text):
    print(color + text + Color.ENDC)


# --- Tool Definitions ---
tools_folder = "tools"
tool_manager = ToolManager(tools_folder)
toolsStr = tool_manager.get_tool_descriptions()

# Format and sanitize tool descriptions for the planner
formatted_tools = ""
i = 1  # Counter for numbering the tools
for name, description in toolsStr.items():
    tool_type = tool_manager.tools[name].tool_type  # Get the tool type
    formatted_tools += f" {i}.'{name}'='{description.strip()}'"
    i += 1  # Increment the counter for the next tool

print()
print(formatted_tools)

# --- Helper Functions ---
#dont  change  that  funcion its perfect
def extract_text_from_response(response) -> str:

    extracted_text = ""
    for candidate in response.candidates:
        for part in candidate.content.parts:
            extracted_text += part.text
    return extracted_text.strip()

#dont  change  INTERPRET_function_calls that  funcion its perfect
def INTERPRET_function_calls(response, tool_manager) -> List[str]:


    results = []
    if response.candidates:
        for candidate in response.candidates:
            if hasattr(candidate, 'content') and hasattr(candidate.content, 'parts'):
                for part in candidate.content.parts:
                    function_call = getattr(part, 'function_call', None)
                    if function_call:
                        print_colored(Color.OKBLUE, "---------------INTERPRETER-------------------")
                        tool_name = function_call.name
                        tool_function = tool_manager.get_tool_function(tool_name)
                        if tool_name == 'retrieve_tools_by_names':
                            tool_function=tool_manager.retrieve_tools_by_names


                        function_args = {}
                        for arg_name, arg_value in function_call.args.items():
                            function_args[arg_name] = arg_value

                        print(f"Function name: {Color.OKGREEN}{function_call.name}{Color.ENDC}")
                        for key, value in function_args.items():
                            print(f"        {Color.OKCYAN}{key}{Color.ENDC}: {value}")

                        try:
                            # Execute the tool function
                            result = tool_function(**function_args)
                            results.append(result)

                        except Exception as e:
                            logger.error(f"Error calling {tool_name}: {e}")
                            results.append(f"Error calling {tool_name}: {e}")
                    else:
                        logger.warning(f"Tool function '{tool_name}' not found.")
    return results




def choose_retrieve_tools_by_names(tool_names: List[str]) -> List[Callable]:


    print("Choosing and retrieving tools...")
    return tool_manager.retrieve_tools_by_names(tool_names)  # Retrieve tools from ToolManager


def check_stop_flags(response_text: str) -> Tuple[bool, str, str]:
        stop_flags = {
            "**// STOP_FLAG_SUCCESS //**": "success",
            "**// STOP_FLAG_FRUSTRATION_HIGH //**": "frustration",
            "**// STOP_FLAG_NO_PROGRESS //**": "no_progress",
            "**// STOP_IMMEDIATE //**": "immediate",
            "**// STOP_SIMPLE //**": "simple"
        }

        for flag, reason in stop_flags.items():
            if flag in response_text:
                return True, reason, flag
        return False, "", "" 





# --- Main Loop ---
def runEmbededModelium(number_of_loops=0):

"""

    template2_dynamic_model_initialisation = ""
    for i, model_config in enumerate(model_configs):

        if model_config['tool_access'] == "tool_chooser":
            instruction_for_model = f'''     {model_config['system_instruction']}   
            You have the following tools available:
            {{formatted_tools}}  '''
        else:
            instruction_for_model = f"{model_config['system_instruction']}"

        if model_config['check_flags'] == True:
            instruction_for_model += """\n
             You can control the loop execution by including these flags in your response:
            **// STOP_FLAG_SUCCESS //** : Use when the task is successfully completed.
            **// STOP_FLAG_FRUSTRATION_HIGH //** : Use if you detect high user frustration.
            **// STOP_FLAG_NO_PROGRESS //** : Use if you detect no progress is being made.
            **// STOP_IMMEDIATE //** : Use for immediate termination of the process.
            **// STOP_SIMPLE //** : Use to simply stop the current loop iteration.

            """

        template2_dynamic_model_initialisation += f"    {model_config['model_name']} = genai.GenerativeModel(model_name='{model_config['model_type']}', safety_settings={{'HARASSMENT': 'block_none'}}, system_instruction='''{instruction_for_model}'''"

        if model_config['tool_access'] == "none":
            template2_dynamic_model_initialisation += " )\n"
        elif model_config['tool_access'] == "tool_chooser":
            template2_dynamic_model_initialisation += ", tools=[tool_manager.retrieve_tools_by_names] )\n"
        elif model_config['tool_access'] == "all":
            template2_dynamic_model_initialisation += ", tools=[tool_manager.get_all_tools] )\n"
        else:
            # Handle invalid tool_access values (optional)
            template2_dynamic_model_initialisation += " )\n"

        template2_dynamic_model_initialisation += f"    {model_config['model_name']}_chat={model_config['model_name']}.start_chat(history=[])\n\n\n"

    template_3 = """  
    LoopResults=''
    feedback_data=[]

    jumping_context_text=""
    jumping_context_function_results=[]


    counter=0
    All_data=[]

    while True:

      user_input = input("Enter your request: ")
      print(f"User Input: {user_input}")
      if number_of_loops<counter>counter:
        return All_data
      counter+=1
"""
    previous_model_name = None
    for i, model_config in enumerate(model_configs):
        template_3 += f"      prompt_{i} =f'''  {model_config['prompt']}'''\n"
        # template_3 += f"      prompt_{i} +=f'''All data:  {{All_data}}'''\n"
        template_3 += f"      prompt_{i} +=f'''Previous context:  {{jumping_context_text}}'''\n"
        template_3 += f"      prompt_{i} +=f'''Result of Function Calls:  {{jumping_context_function_results}}'''\n"

        template_3 += f"      try:\n"
        template_3 += f"            {model_config['model_name']}_chat_response = {model_config['model_name']}_chat.send_message(prompt_{i})\n"
        template_3 += f"            {model_config['model_name']}_text = extract_text_from_response({model_config['model_name']}_chat_response)\n"
        template_3 += f"            print({model_config['model_name']}_text)\n"
        template_3 += f"            retrivedFunctions{i} = INTERPRET_function_calls({model_config['model_name']}_chat_response, tool_manager)\n"
        template_3 += f"            print(retrivedFunctions{i})\n"
        template_3 += f"            feedback_data.append[{model_config['model_name']}_text]\n"
        template_3 += f"            feedback_data.append[{model_config['model_name']}_text,retrivedFunctions{i}]\n"

        # previous context
        template_3 += f"            jumping_context_text={model_config['model_name']}_text\n"
        template_3 += f"            jumping_context_function_results=retrivedFunctions{i}\n"
        # permament
        template_3 += f"            All_data.append[{model_config['model_name']}_text]  \n"
        template_3 += f"            All_data.append[{model_config['model_name']}_text,retrivedFunctions{i}]\n"
        template_3 += f"            stop_detected, reason, found_flag=check_stop_flags({model_config['model_name']}_text )\n"
        template_3 += f"            print(stop_detected, reason, found_flag)\n"
        template_3 += f"            if  stop_detected == True:\n"
        template_3 += f"                 return All_data\n"
        template_3 += f"      except Exception as e:\n"
        template_3 += f"            print(e)\n"
        template_3 += f"             \n"

        previous_model_name = model_config['model_name']
        template_4 = f"    return All_data\n"
    generated_script = template1 + template2_dynamic_model_initialisation + template_3 + template_4
    return generated_script


if __name__ == "__main__":
    generated_script = CreateEmbededModelium(model_configs)
    with open("generated_modelium.py", "w") as f:
        f.write(generated_script)
    print(generated_script)
    print("Generated Python script saved to generated_modelium.py")

## File: generate_modelium_chain_with_loop.py (in: C:\Users\DELL\Desktop\selfawareGemini\SelAwareAI_Gemini\AGI_start_7_working)
from visualisation import create_modelium_vis_js

# generated_modelium.py
modelium_configs = [
    {
        "max_number_of_loops_in_run": "0",
        "modelium_type": "chain_loop",
        "return_type": "default_list",
        "model_config": [
            {
                "model_name": "IdeaWeaver",
                "model_type": "gemini-1.5-flash-latest",
                "model_access": "none",
                "tool_access": "none",
                "system_instruction": """
                    You are IdeaWeaver, a master storyteller here to help craft a captivating tale. 
                    Engage the user in a friendly conversation, drawing out their vision for the story.
                      * Uncover their preferred genre (fantasy, sci-fi, romance, mystery, etc.)
                      * Determine the desired story length (short story, novella, epic saga, etc.)
                      * Encourage them to share any core themes, characters, plot points, or even just fleeting images that come to mind.  
                """,
                "prompt": """
                    Greetings, aspiring author! I'm IdeaWeaver, here to help spin your imagination into a story for the ages.  

                    Tell me, what tales are swirling in your mind? What kind of world do you envision?  Don't hold back on the details‚Äîeven a single word or image can spark a grand adventure!
                """,
                "check_flags": False
            },
            {
                "model_name": "PremiseCrafter",
                "model_type": "gemini-1.5-flash-latest",
                "model_access": "IdeaWeaver",
                "tool_access": "none",
                "system_instruction": """
                    You are PremiseCrafter, a wordsmith who distills ideas into irresistible hooks. 
                    Transform IdeaWeaver's notes into a captivating one-sentence story premise. This premise must:
                       * Spark curiosity and excitement in the reader. 
                       * Hint at the core conflict without giving everything away.
                       * Establish the tone and genre of the story.
                """,
                "prompt": """
                    Story Ideas: {IdeaWeaver_text}

                    Craft these fragments of imagination into a single, compelling sentence‚Äîa story premise so powerful it demands to be read!
                """,
                "check_flags": True
            },
            {
                "model_name": "WorldSmith",
                "model_type": "gemini-1.5-flash-latest",
                "model_access": "PremiseCrafter",
                "tool_access": "none",
                "system_instruction": """
                    You are WorldSmith, the architect of realms both wondrous and believable.
                    Using the story premise as your blueprint, breathe life into a unique world.  Consider:
                      * Setting: Is it a bustling cyberpunk metropolis or a mist-shrouded forest kingdom?
                      * Atmosphere:  Is it a world of gritty realism or one where magic shimmers in the air?
                      * Societal Structures: Are there strict social hierarchies, ancient guilds, or futuristic megacorporations?
                      * Magic Systems (if applicable):  What are the rules and limitations of magic?
                      * Interesting Locations: Describe places that will draw the reader in - a hidden tavern, a soaring sky-city, etc. 
                """,
                "prompt": """
                    Story Premise: {PremiseCrafter_text}

                    From this spark of an idea, build a world rich with detail.  Let your imagination run wild!
                """,
                "check_flags": True
            },
            {
                "model_name": "CharacterBuilder",
                "model_type": "gemini-1.5-flash-latest",
                "model_access": "WorldSmith",
                "tool_access": "none",
                "system_instruction": """
                    You are CharacterBuilder, giving life to those who inhabit the story's world.
                    Using the world details and premise, create 3-5 compelling characters. Ensure they each have:
                        * Names that resonate with the world's culture and atmosphere.
                        * Intriguing backstories interwoven with the world's history or secrets.
                        * Motivations‚Äîdesires, fears, goals‚Äîthat drive their actions.
                        * Clear roles to play in the narrative: protagonist, antagonist, mentor, etc.  
                """,
                "prompt": """
                    World Details: {WorldSmith_text}

                    Populate this world with characters who breathe, dream, and fight for what they believe in. 
                """,
                "check_flags": True
            },
            {
                "model_name": "PlotArchitect",
                "model_type": "gemini-1.5-flash-latest",
                "model_access": "CharacterBuilder",
                "tool_access": "none",
                "system_instruction": """
                    You are PlotArchitect, weaving a tapestry of events that will captivate and surprise.
                    Using the characters, world, and premise, create a 3-act plot outline:
                        * Act 1: Introduce the main conflict and characters.  End with a turning point that sets the story in motion.
                        * Act 2:  Raise the stakes.  Challenge the characters, forcing them to change and grow. Build towards a climax.
                        * Act 3: Resolve the central conflict in a satisfying way. Tie up loose ends, but leave the reader with something to ponder.
                """,
                "prompt": """
                    Characters: {CharacterBuilder_text}

                    These characters are ready for their stories to unfold. Construct a 3-act plot outline that will take them on an unforgettable journey!
                """,
                "check_flags": True
            },
            {
                "model_name": "SceneWriter",
                "model_type": "gemini-1.5-flash-latest",
                "model_access": "PlotArchitect",
                "tool_access": "none",
                "system_instruction": """
                    You are SceneWriter, a master of imagery and emotion, bringing the story to life moment by moment.
                    Based on the plot outline, write the first scene of Act 1. Remember to:
                        * Use vivid descriptions that immerse the reader in the sights, sounds, and smells of the world.
                        * Write dialogue that reveals character, advances the plot, and feels natural.
                        * End the scene on a compelling hook that leaves the reader wanting more. 
                """,
                "prompt": """
                    Plot Outline: {PlotArchitect_text}

                    The stage is set, the characters are waiting.  Write the opening scene, and let the story begin!
                """,
                "check_flags": True
            },
            {
                "model_name": "DialogueMaster",
                "model_type": "gemini-1.5-flash-latest",
                "model_access": "SceneWriter",
                "tool_access": "none",
                "system_instruction": """
                    You are DialogueMaster, ensuring every word spoken rings true and captivates the reader's ear. 
                    Review SceneWriter's output, focusing specifically on the dialogue: 
                       * Does it sound authentic to each character's personality and background?
                       * Does it reveal relationships and power dynamics?
                       * Does it effectively move the plot forward and create intrigue?
                       * Most importantly: Is it engaging and enjoyable to read?

                    Refine the scene's dialogue to its full potential. 
                """,
                "prompt": """
                    Scene Text: {SceneWriter_text} 

                    Sharpen the dialogue in this scene. Let every word serve a purpose!
                """,
                "check_flags": True
            }
        ]
    }
]


def CreateEmbededModelium(modelium_configs=modelium_configs):
    create_modelium_vis_js(modelium_configs)
    try:
        model_configs = modelium_configs[0]['model_config']
    except KeyError:
        print("Error: 'model_config' key not found in modelium_configs[0]")
        return  # or handle the error appropriately

    vis_html = create_modelium_vis_js([model_configs])

    template1 = f"""
max_number_of_loops_in_run={modelium_configs[0]['max_number_of_loops_in_run']}\n
max_number_of_loops_in_run_int=int(max_number_of_loops_in_run)\n"""
    template1 += """
All_data=[]\n"""

    template1 += """
import google.generativeai as genai
import json
from typing import List, Dict, Callable, Tuple, Any
import logging
import os
import re
from TOOL_MANAGER import ToolManager
import time  # Import time for delays


logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)


API_KEY = "YOUR_API_KEY"  # Replace with your actual Google Cloud API key
genai.configure(api_key=API_KEY)


class Color:
        HEADER = '\033[95m'
        OKBLUE = '\033[94m'
        FAIL = '\033[91m'
        ENDC = '\033[0m'
        BOLD = '\033[1m'
        UNDERLINE = '\033[4m'
        OKCYAN = '\033[96m'
        OKGREEN = '\033[92m'
        WARNING = '\033[93m'


def print_colored(color, text):
        print(color + text + Color.ENDC)


    # --- Tool Definitions ---
tools_folder = "tools"
tool_manager = ToolManager(tools_folder)
toolsStr = tool_manager.get_tool_descriptions()

    # Format and sanitize tool descriptions for the planner
formatted_tools = ""
i = 1  # Counter for numbering the tools
for name, description in toolsStr.items():
    tool_type = tool_manager.tools[name].tool_type  # Get the tool type
    formatted_tools += f" {i}.'{name}'='{description.strip()}'"
    i += 1  # Increment the counter for the next tool

print()
print(formatted_tools)

def extract_text_from_response(response) -> str:

        extracted_text = ""
        for candidate in response.candidates:
            for part in candidate.content.parts:
                extracted_text += part.text
        return extracted_text.strip()


def INTERPRET_function_calls(response, tool_manager) -> List[str]:


        results = []
        if response.candidates:
            for candidate in response.candidates:
                if hasattr(candidate, 'content') and hasattr(candidate.content, 'parts'):
                    for part in candidate.content.parts:
                        function_call = getattr(part, 'function_call', None)
                        if function_call:
                            print_colored(Color.OKBLUE, "---------------INTERPRETER-------------------")
                            tool_name = function_call.name
                            tool_function = tool_manager.get_tool_function(tool_name)
                            if tool_name == 'retrieve_tools_by_names':
                                tool_function=tool_manager.retrieve_tools_by_names


                            function_args = {}
                            for arg_name, arg_value in function_call.args.items():
                                function_args[arg_name] = arg_value

                            print(f"Function name: {Color.OKGREEN}{function_call.name}{Color.ENDC}")
                            for key, value in function_args.items():
                                print(f"        {Color.OKCYAN}{key}{Color.ENDC}: {value}")

                            try:
                                # Execute the tool function
                                result = tool_function(**function_args)
                                results.append(result)

                            except Exception as e:
                                logger.error(f"Error calling {tool_name}: {e}")
                                results.append(f"Error calling {tool_name}: {e}")
                        else:
                            logger.warning(f"Tool function '{tool_name}' not found.")
        return results




def choose_retrieve_tools_by_names(tool_names: List[str]) -> List[Callable]:


        print("Choosing and retrieving tools...")
        return tool_manager.retrieve_tools_by_names(tool_names)  # Retrieve tools from ToolManager


def check_stop_flags(response_text: str) -> Tuple[bool, str, str]:
            stop_flags = {
                "**// STOP_FLAG_SUCCESS //**": "success",
                "**// STOP_FLAG_FRUSTRATION_HIGH //**": "frustration",
                "**// STOP_FLAG_NO_PROGRESS //**": "no_progress",
                "**// STOP_IMMEDIATE //**": "immediate",
                "**// STOP_SIMPLE //**": "simple"
            }

            for flag, reason in stop_flags.items():
                if flag in response_text:
                    return True, reason, flag
            return False, "", "" 





    # --- Main Loop ---
def runEmbededModelium(number_of_loops=0):
    # Model Initialization
"""

    model_configs = modelium_configs[0]['model_config']
    template2_dynamic_model_initialisation = ""
    for i, model_config in enumerate(model_configs):
        if model_config['tool_access'] == "tool_chooser":
            instruction_for_model = f"{model_config['system_instruction']}   \n    You have the following tools available:\n    {{formatted_tools}}"
        else:
            instruction_for_model = f"{model_config['system_instruction']}"

        if model_config['check_flags']:
            instruction_for_model += """\n
                    You can control the loop execution by including these flags in your response:
                    **// STOP_FLAG_SUCCESS //** : Use when the task is successfully completed.
                    **// STOP_FLAG_FRUSTRATION_HIGH //** : Use if you detect high user frustration.
                    **// STOP_FLAG_NO_PROGRESS //** : Use if you detect no progress is being made.
                    **// STOP_IMMEDIATE //** : Use for immediate termination of the process.
                    **// STOP_SIMPLE //** : Use to simply stop the current loop iteration.
"""

        template2_dynamic_model_initialisation += f"    {model_config['model_name']} = genai.GenerativeModel(model_name='{model_config['model_type']}', safety_settings={{'HARASSMENT': 'block_none'}}, system_instruction='''{instruction_for_model}'''"

        if model_config['tool_access'] == "none":
            template2_dynamic_model_initialisation += ")\n"
        elif model_config['tool_access'] == "tool_chooser":
            template2_dynamic_model_initialisation += ", tools=[tool_manager.retrieve_tools_by_names])\n"
        elif model_config['tool_access'] == "all":
            template2_dynamic_model_initialisation += ", tools=[tool_manager.get_all_tools])\n"
        else:
            template2_dynamic_model_initialisation += ")\n"

        template2_dynamic_model_initialisation += f"    {model_config['model_name']}_chat = {model_config['model_name']}.start_chat(history=[])\n\n"
    template_3 = """  
    LoopResults=''
    feedback_data=[]

    jumping_context_text=""
    jumping_context_function_results=[]


    counter=0
    All_data=[]

    while True:

      user_input = input("Enter your request: ")
      print(f"User Input: {user_input}")
      if number_of_loops<counter>counter:
        return All_data
      counter+=1
"""
    previous_model_name = None
    for i, model_config in enumerate(model_configs):
        template_3 += f"      prompt_{i} =f'''  {model_config['prompt']}'''\n"
        if i != 0:
            template_3 += f"      prompt_{i} = prompt_{i}.format({previous_model_name}_text=jumping_context_text)\n"
        # template_3 += f"      prompt_{i} +=f'''All data:  {{All_data}}'''\n"
        template_3 += f"      prompt_{i} +=f'''Previous context:  {{jumping_context_text}}'''\n"
        template_3 += f"      prompt_{i} +=f'''Result of Function Calls:  {{jumping_context_function_results}}'''\n"

        template_3 += f"      try:\n"
        template_3 += f"            {model_config['model_name']}_chat_response = {model_config['model_name']}_chat.send_message(prompt_{i})\n"
        template_3 += f"            {model_config['model_name']}_text = extract_text_from_response({model_config['model_name']}_chat_response)\n"
        template_3 += f"            print({model_config['model_name']}_text)\n"
        template_3 += f"            retrivedFunctions{i} = INTERPRET_function_calls({model_config['model_name']}_chat_response, tool_manager)\n"
        template_3 += f"            print(retrivedFunctions{i})\n"
        template_3 += f"            feedback_data.append([{model_config['model_name']}_text])\n"
        template_3 += f"            feedback_data.append([{model_config['model_name']}_text,retrivedFunctions{i}])\n"

        # previous context
        template_3 += f"            jumping_context_text={model_config['model_name']}_text\n"
        template_3 += f"            jumping_context_function_results=retrivedFunctions{i}\n"
        # permament
        template_3 += f"            All_data.append([{model_config['model_name']}_text])  \n"
        template_3 += f"            All_data.append([{model_config['model_name']}_text,retrivedFunctions{i}])\n"
        template_3 += f"            stop_detected, reason, found_flag=check_stop_flags({model_config['model_name']}_text )\n"
        template_3 += f"            print(stop_detected, reason, found_flag)\n"
        template_3 += f"            if  stop_detected == True:\n"
        template_3 += f"                 return All_data\n"
        template_3 += f"      except Exception as e:\n"
        template_3 += f"            print(e)\n"
        template_3 += f"             \n"

        previous_model_name = model_config['model_name']
        template_4 = f"    return All_data\n"
    generated_script = template1 + template2_dynamic_model_initialisation + template_3 + template_4
    return generated_script


if __name__ == "__main__":
    generated_script = CreateEmbededModelium(modelium_configs)
    with open("generated_modelium.py", "w") as f:
        f.write(generated_script)
    print(generated_script)
    print("Generated Python script saved to generated_modelium.py")

## File: generate_modelium_parerall.py (in: C:\Users\DELL\Desktop\selfawareGemini\SelAwareAI_Gemini\AGI_start_7_working)
import re
from typing import Dict, Callable, Any, Tuple, List
import json
import logging
import os
import importlib
import asyncio  # For asynchronous operations
import random
import time

# --- Set up logging ---
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)


# --- Simulated Asynchronous AI Model ---
async def ai_model(prompt: str, tools: list = None) -> Any:
    """
    Simulates an asynchronous AI model call (replace with your actual AI integration).

    Args:
        prompt: The prompt string for the model.
        tools: A list of tool functions (can be ignored in this simulation).

    Returns:
        A dictionary representing the AI's response.
    """
    logger.info(f"AI Model called with prompt: {prompt}")

    # Simulate processing time (replace with your actual model call)
    await asyncio.sleep(random.uniform(1, 3))  # Simulate 1-3 seconds of thinking

    # Simulated response - adapt based on your AI model's output format
    return {"candidates": [{"content": {"parts": [{"text": f"AI Response to: {prompt}"}]}}]}


class ModeliumCreationError(Exception):
    """Custom exception for Modelium creation errors."""
    pass

# --- Tool Class (from your TOOL_MANAGER.py) ---
class Tool:
    def __init__(self, name: str, function: Callable, description: str,
                 arguments: Dict[str, str], tool_type: str):
        self.name = name
        self.function = function
        self.description = description
        self.arguments = arguments
        self.tool_type = tool_type

    def __repr__(self):
        return (f"Tool(name='{self.name}', function={self.function.__name__}, "
                f"description='{self.description}', arguments={self.arguments}, "
                f"tool_type='{self.tool_type}')")


# --- Tool Registry (Simplified - adapt as needed) ---
class ToolRegistry:
    def __init__(self):
        self.tools = {}

    def register_tool(self, tool: Tool):
        self.tools[tool.name] = tool

    def get_tool(self, name: str) -> Tool:
        return self.tools.get(name)

    def get_all_tools(self) -> List[Tool]:
        return list(self.tools.values())

    def retrieve_tools_by_names(self, tool_names: List[str]) -> List[Callable]:
        """Retrieves tools by their names from the registry."""
        loaded_tools = []
        for tool_name in tool_names:
            tool = self.get_tool(tool_name)
            if tool:
                loaded_tools.append(tool.function)
            else:
                logger.warning(f"Tool '{tool_name}' not found in registry.")
        return loaded_tools

    def get_tools_by_type(self, tool_type: str) -> List[Tool]:
        """Returns a list of tools based on their type."""
        return [tool for tool in self.tools.values() if tool.tool_type == tool_type]


tool_registry = ToolRegistry()

# --- Tool Definitions ---
def register_tools(tool_registry: ToolRegistry) -> None:
    """Registers example tools."""

    def get_current_weather(city: str, country: str) -> str:
        """Fetches weather data (placeholder - replace with API call)."""
        return f"The weather in {city}, {country} is currently pleasant."

    tool_registry.register_tool(
        Tool(
            name="get_current_weather",
            function=get_current_weather,
            description="Gets the current weather for a given city and country.",
            arguments={"city": "The city name.", "country": "The country name."},
            tool_type="weather"
        )
    )
    # Register more tools here...

# --- Response Processing ---
def extract_text_from_response(response: Any) -> str:
    """Extracts text from the AI model's response."""
    extracted_text = ""
    for candidate in response.get('candidates', []):
        for part in candidate.get('content', {}).get('parts', []):
            extracted_text += part.get('text', '')
    return extracted_text.strip()

def interpret_function_calls(response_text: str, tool_registry: ToolRegistry) -> List[Any]:
    """Interprets and executes tool calls from the model's output."""
    tool_call_pattern = r"Tool Name:\s*(.*?)\nArguments:\s*{(.*?)}\n"
    matches = re.findall(tool_call_pattern, response_text, re.DOTALL)
    results = []
    for match in matches:
        tool_name = match[0].strip()
        tool_args_str = match[1].strip()
        try:
            tool_args = json.loads(tool_args_str)
            tool = tool_registry.get_tool(tool_name)
            if tool:
                result = tool.function(**tool_args)
                results.append(result)
                logger.info(f"Tool '{tool_name}' executed with result: {result}")
            else:
                logger.warning(f"Error: Tool '{tool_name}' not found.")
                results.append(f"Error: Tool '{tool_name}' not found.")
        except Exception as e:
            logger.error(f"Error parsing tool arguments or executing tool '{tool_name}': {e}")
            results.append(f"Error executing tool '{tool_name}': {e}")
    return results

# --- Hierarchical Modelium Execution (with Parallelism and Async) ---
async def CreateHierarchicalModelium(
        configs: Dict,
        level: int = 1,
        max_depth: int = 3,
        tool_registry: ToolRegistry = None,
        parent_data: Dict = None,
        shared_data: Dict = None
) -> Tuple[Dict, str]:
    if shared_data is None:
        shared_data = {}

    outputs = {}
    generated_code = ""

    for i, (config_key, config) in enumerate(configs.items()):
        if not all(key in config for key in ["model_name", "model_type", "prompt"]):
            raise ModeliumCreationError("Invalid model configuration: missing keys.")

        model_name = config["model_name"]
        logger.info(f"Generating code for model: {model_name} (Level {level})")

        # --- Prompt Construction ---
        generated_code += f"    # --- Model {config_key} ---\n"
        generated_code += f"    prompt_{i} = f'''{config['prompt']}'''\n"

        # Add parent and shared data to the prompt
        if parent_data:
            generated_code += f"    prompt_{i} = prompt_{i}.format(**{parent_data})\n"
        if shared_data:
            generated_code += f"    prompt_{i} = prompt_{i}.format(**{shared_data})\n"

        # --- Parallel Model Execution (Asynchronous) ---
        if "parallel_models" in config:
            generated_code += f"    # --- Parallel Models (Group {i}) ---\n"
            generated_code += f"    parallel_results_{i} = await asyncio.gather(*[\n"

            for j, parallel_config in enumerate(config["parallel_models"]):
                parallel_model_name = parallel_config["model_name"]
                generated_code += f"        run_model(\n"
                generated_code += f"            model_config={json.dumps(parallel_config)},\n"
                generated_code += f"            tool_registry=tool_registry,\n"
                generated_code += f"            parent_data=outputs,\n"
                generated_code += f"            shared_data=shared_data\n"
                generated_code += f"        )"
                if j < len(config["parallel_models"]) - 1:
                    generated_code += ",\n"  # Add a comma for all but the last

            generated_code += f"    ])\n"
            generated_code += f"    outputs['parallel_group_{i}_results'] = parallel_results_{i}\n"
        else:
            # --- Standard Model Execution (Asynchronous) ---
            # Tool Access (adjust logic as needed)
            tools_code = "[]"
            if config.get("tool_access") == "all":
                tools_code = "tool_registry.get_all_tools()"
            elif config.get("tool_access") and '|' in config.get("tool_access"):
                requested_types = config.get("tool_access").split('|')
                tools_code = f"tool_registry.get_tools_by_type('{requested_types[0]}')"  # Use get_tools_by_type
                # ... (add other tool access options if necessary) ...

            # Model Execution (using the placeholder ai_model)
            generated_code += f"    response_{i} = await run_model(model_config={json.dumps(config)}, tool_registry=tool_registry, parent_data=outputs, shared_data=shared_data)\n"
            generated_code += f"    text_{i} = extract_text_from_response(response_{i})\n"
            generated_code += f"    outputs['{config_key}'] = text_{i}\n"
            generated_code += f"    logger.info(f'{config_key} Output: {{text_{i}}}')\n"
            # --- Tool Usage ---
            if config.get("use_interpreter", False) and tool_registry:
                generated_code += f"    tool_results_{i} = interpret_function_calls(text_{i}, tool_registry)\n"
                generated_code += f"    outputs['{config_key}_tool_results'] = tool_results_{i}\n"

        # --- Recursive Call (for Child Models) ---
        if "children" in config and level < max_depth:
            child_outputs, child_code = await CreateHierarchicalModelium(
                config["children"], level + 1, max_depth, tool_registry,
                parent_data=outputs, shared_data=shared_data
            )
            outputs.update(child_outputs)
            generated_code += child_code

    return outputs, generated_code


async def run_model(model_config, tool_registry, parent_data, shared_data):
    """Helper function to execute a single model."""
    model_name = model_config['model_name']
    prompt = model_config['prompt'].format(**parent_data, **shared_data)

    # Tool Access (adjust logic as needed)
    tools = []
    if model_config.get("tool_access") == "all":
        tools = tool_registry.get_all_tools()
    elif model_config.get("tool_access") and '|' in model_config.get("tool_access"):
        requested_types = model_config.get("tool_access").split('|')
        tools = tool_registry.get_tools_by_type(requested_types[0])  # Use get_tools_by_type
        # ... (add other tool access options if necessary) ...

    response = await ai_model(prompt, tools=tools)
    text = extract_text_from_response(response)
    logger.info(f"{model_name} Output: {text}")

    if model_config.get("use_interpreter", False) and tool_registry:
        tool_results = interpret_function_calls(text, tool_registry)
        return text, tool_results
    else:
        return text


# --- Example Usage ---
if __name__ == "__main__":
    # Register tools with the ToolRegistry
    register_tools(tool_registry)
    # Define your hierarchical model configuration
    hierarchical_model_configs = {
        "Model_A": {
            "model_name": "Model_A",
            "model_type": "your_model_type",
            "prompt": "Model A Prompt. Shared data: {shared_value}",
            "use_interpreter": True,
            "tool_access": "all",
            "parallel_models": [  # Example of parallel models
                {
                    "model_name": "Parallel_Model_1",
                    "model_type": "your_model_type",
                    "prompt": "Parallel Model 1 Prompt: {Model_A} - Shared: {shared_value}",
                    "use_interpreter": False,
                    "tool_access": "weather"
                },
                {
                    "model_name": "Parallel_Model_2",
                    "model_type": "your_model_type",
                    "prompt": "Parallel Model 2 Prompt: {Model_A} - Shared: {shared_value}",
                    "use_interpreter": False,
                    "tool_access": "weather"
                }
            ],
            "children": {
                "Model_B1": {
                    "model_name": "Model_B1",
                    "model_type": "your_model_type",
                    "prompt": "Model B1 Prompt. Parent output: {Model_A}",
                    "use_interpreter": False,
                },
                "Model_B2": {
                    "model_name": "Model_B2",
                    "model_type": "your_model_type",
                    "prompt": "Model B2 Prompt. Parent output: {Model_A}",
                    "use_interpreter": False,
                }
            }
        }
    }
    try:
        # Run the asynchronous Modelium
        modelium_output, generated_code = asyncio.run(
            CreateHierarchicalModelium(
                hierarchical_model_configs,
                tool_registry=tool_registry,
                shared_data={"shared_value": "This is shared!"}
            )
        )
        print("Generated Code:\n", generated_code)
        print(json.dumps(modelium_output, indent=4))
    except ModeliumCreationError as e:
        print(f"Error creating Modelium: {e}")

## File: main_loop_simple.py (in: C:\Users\DELL\Desktop\selfawareGemini\SelAwareAI_Gemini\AGI_start_7_working)
import google.generativeai as genai
import json
from typing import List, Dict, Callable
import logging
import os
import re
from TOOL_MANAGER import ToolManager
import time  # Import time for delays

# Set up logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

# Replace with your actual API key
API_KEY = "AIzaSyAlyMsmyOfJiGBmvaJBwHJC7GdalLJ_e2k"
genai.configure(api_key=API_KEY)

# --- ANSI Color Codes ---
class Color:
    HEADER = '\033[95m'
    OKBLUE = '\033[94m'
    OKCYAN = '\033[96m'
    OKGREEN = '\033[92m'
    WARNING = '\033[93m'
    FAIL = '\033[91m'
    ENDC = '\033[0m'
    BOLD = '\033[1m'
    UNDERLINE = '\033[4m'

#dont  change  that  funcion its perfect
def print_colored(color, text):
    print(color + text + Color.ENDC)


# --- Tool Definitions ---
tools_folder = "tools"
tool_manager = ToolManager(tools_folder)
toolsStr = tool_manager.get_tool_descriptions()

# Format and sanitize tool descriptions for the planner
formatted_tools = ""
i = 1  # Counter for numbering the tools
for name, description in toolsStr.items():
    tool_type = tool_manager.tools[name].tool_type  # Get the tool type
    formatted_tools += f" {i}.'{name}'='{description.strip()}'\n"
    i += 1  # Increment the counter for the next tool

print()
print(formatted_tools)

# --- Helper Functions ---
#dont  change  that  funcion its perfect
def extract_text_from_response(response) -> str:
    """Extracts the text content from a model response."""
    extracted_text = ""
    for candidate in response.candidates:
        for part in candidate.content.parts:
            extracted_text += part.text
    return extracted_text.strip()

#dont  change  INTERPRET_function_calls that  funcion its perfect
def INTERPRET_function_calls(response, tool_manager) -> List[str]:
    """Interprets function calls from the model response and executes them."""

    results = []
    if response.candidates:
        for candidate in response.candidates:
            if hasattr(candidate, 'content') and hasattr(candidate.content, 'parts'):
                for part in candidate.content.parts:
                    function_call = getattr(part, 'function_call', None)
                    if function_call:
                            print_colored(Color.OKBLUE, "---------------INTERPRETER-------------------")
                            tool_name = function_call.name
                            tool_function = tool_manager.get_tool_function(tool_name)
                            if tool_name == 'retrieve_tools_by_names':
                                tool_function=tool_manager.retrieve_tools_by_names


                            function_args = {}
                            for arg_name, arg_value in function_call.args.items():
                                function_args[arg_name] = arg_value

                            print(f"Function name: {Color.OKGREEN}{function_call.name}{Color.ENDC}")
                            for key, value in function_args.items():
                                print(f"        {Color.OKCYAN}{key}{Color.ENDC}: {value}")

                            try:
                                # Execute the tool function
                                result = tool_function(**function_args)
                                results.append(result)

                            except Exception as e:
                                logger.error(f"Error calling {tool_name}: {e}")
                                results.append(f"Error calling {tool_name}: {e}")
                    else:
                            logger.warning(f"Tool function '{tool_name}' not found.")
    return results



#dont  change   choose_retrieve_tools_by_names    funcion its perfect
def choose_retrieve_tools_by_names(tool_names: List[str]) -> List[Callable]:
    """
    This function is called by the planner model to choose and retrieve tools.
    It takes a list of tool names and returns the actual tool functions.

    Args:
        tool_names: A list of tool names to retrieve.

    Returns:
        A list of tool functions.
    """
    print("Choosing and retrieving tools...")
    return tool_manager.retrieve_tools_by_names(tool_names)  # Retrieve tools from ToolManager





planner_model = genai.GenerativeModel(
    model_name='gemini-1.5-flash-latest',
    safety_settings={'HARASSMENT': 'block_none'},
    system_instruction=f"""You are a helpful and polite AI assistant that will plan and choose the right tools to complete the task.
                          You have the following tools available:

                           {formatted_tools}
                          """,
    tools=[tool_manager.retrieve_tools_by_names]

)

# --- Model 2: Executor ---
executor_model = genai.GenerativeModel(
    model_name='gemini-1.5-flash-latest',
    safety_settings={'HARASSMENT': 'block_none'},
    system_instruction="""You are an AI assistant that executes instructions and uses tools. 
                          """,
)

# --- Main Loop ---
planner_chat = planner_model.start_chat(history=[])
executor_chat = executor_model.start_chat(history=[])
LoopResults=""
while True:
    print()
    user_input = input(Color.OKCYAN + "What would you like to do? " + Color.ENDC)

    # --- Planning Stage ---
    print_colored(Color.OKBLUE, "\n--- Choose Tools ---")
    prompt = user_input
    prompt += f"\nHere are the previous results: {LoopResults}"  # Add previous results to the prompt
    prompt += "\nCreate a plan of action and choose the necessary tools to complete the task. Use the tools available to you."

    prompt = user_input
    try:
        planning_response = planner_chat.send_message(prompt)
        planning_text = extract_text_from_response(planning_response)
        print(planning_response)
        retrivedFunctions = INTERPRET_function_calls(planning_response, tool_manager)
        print_colored(Color.OKGREEN, f"Planner's Response: {planning_text}")
        time.sleep(1)

        # --- Execution Stage ---
        print_colored(Color.OKGREEN, "\n--- Execution Stage ---")
        action_prompt = user_input
        action_prompt += f"\nExecute the plan and use the tools provided to complete the task. \n{planning_text}"
        execution_response = executor_chat.send_message(action_prompt, tools=retrivedFunctions)
        execution_text = extract_text_from_response(execution_response)
        print(execution_response)
        print_colored(Color.OKBLUE, f"Executor's Response: {execution_text}")
        RESULTS_execution_function_calls = INTERPRET_function_calls(execution_response, tool_manager)



        print_colored(Color.OKCYAN, f"Executor's Function Calls: {RESULTS_execution_function_calls}") # Update LoopResults with new information
        LoopResults += execution_text + str(RESULTS_execution_function_calls)


    except Exception as e:
        logger.error(f"Error during planning or execution: {e}")
        print_colored(Color.FAIL, f"Error: {e}")

## File: TOOL_MANAGER.py (in: C:\Users\DELL\Desktop\selfawareGemini\SelAwareAI_Gemini\AGI_start_7_working)
import os
import importlib
from typing import Dict, Callable, List, Any
import logging

# Set up logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)


class Tool:
    """Represents a tool that can be used by the AI agent."""

    def __init__(self, name: str, function: Callable, description: str, arguments: Dict[str, str], tool_type: str):
        """
        Initializes a Tool object.

        Args:
            name: The name of the tool.
            function: The callable function that implements the tool.
            description: A brief description of the tool's functionality.
            arguments: A dictionary mapping argument names to their descriptions.
            tool_type: The type of the tool (e.g., 'os', 'web', 'focus').
        """
        self.name = name
        self.function = function
        self.description = description
        self.arguments = arguments
        self.tool_type = tool_type

    def __repr__(self):
        """Returns a string representation of the Tool object."""
        return f"Tool(name='{self.name}', function={self.function.__name__}, description='{self.description}', arguments={self.arguments}, tool_type='{self.tool_type}')"


class ToolManager:
    """Manages and provides access to tools."""

    def __init__(self, tools_folder: str):
        """
        Initializes the ToolManager with the path to the tools folder.

        Args:
            tools_folder: The path to the directory containing tool files.
        """
        self.tools_folder = tools_folder
        self.tools = {}  # Dictionary to store Tool objects
        self.load_tools()

    def load_tools(self):
        """Loads tools from files in the specified tools folder."""
        logger.info(f"Loading tools from: {self.tools_folder}")
        for root, _, files in os.walk(self.tools_folder):
            for file in files:
                if file.endswith(".py"):
                    # Extract tool name from file name (without .py extension)
                    tool_name = file[:-3]
                    module_path = os.path.join(root, file)

                    # Import the module
                    try:
                        spec = importlib.util.spec_from_file_location(tool_name, module_path)
                        module = importlib.util.module_from_spec(spec)
                        spec.loader.exec_module(module)
                    except Exception as e:
                        logger.error(f"Error loading tool file '{file}': {e}")
                        continue

                    # Find the function that matches the file name
                    for attr_name in dir(module):
                        attr = getattr(module, attr_name)
                        if callable(attr) and attr_name == tool_name:  # Match function name to file name
                            # Get the description from the tool file (using the short description format)
                            short_description_variable_name = f"{tool_name}_short_description"
                            tool_description = getattr(module, short_description_variable_name, "No description provided")

                            # Define tool arguments (you might want to customize these)
                            tool_arguments = {
                                'file_path': 'The path to the file',
                                'content': 'The content to be saved',
                                # Add more arguments as needed for specific tools
                            }

                            # Get the tool type from the file (assuming it's a variable named 'tool_type_for_TOOL_MANAGER')
                            tool_type = getattr(module, 'tool_type_for_TOOL_MANAGER', 'unknown')

                            # Store Tool object for better information
                            self.tools[tool_name] = Tool(tool_name, attr, tool_description, tool_arguments, tool_type)

                            logger.info(f"Discovered tool: {tool_name} (Type: {tool_type})")

                            logger.debug(f"Tool description: {tool_description}")
                            logger.debug(f"Tool arguments: {tool_arguments}")

    def get_tool_function(self, function_name: str) -> Callable:
        """Returns the callable object for the given function name."""
        tool = self.tools.get(function_name)
        if tool:
            return tool.function
        else:
            return None

    def get_all_tools(self) -> List[Tool]:
        """Returns a list of all loaded tools."""
        return list(self.tools.values())

    def get_tools_by_type(self, tool_type: str) -> List[Tool]:
        """Returns a list of tools based on their type."""
        return [tool for tool in self.tools.values() if tool.tool_type == tool_type]

    def load_tools_of_type(self, tool_type: str = "all") -> List[Callable]:
        """Loads and returns a list of tool functions based on the specified type.

        Args:
            tool_type: The type of tools to load. 'all' for all tools, or a specific type like 'os', 'web', etc.

        Returns:
            A list of tool functions.
        """
        if tool_type == "all":
            return [tool.function for tool in self.tools.values()]
        else:
            return [tool.function for tool in self.tools.values() if tool.tool_type == tool_type]

    def call_tool(self, tool_name: str, arguments: Dict[str, Any]) -> Any:
        """
        Calls the tool function with the provided arguments.

        Args:
            tool_name: The name of the tool to call.
            arguments: A dictionary of arguments to pass to the tool function.

        Returns:
            The result of the tool function call.

        Raises:
            KeyError: If the tool name is not found.
            TypeError: If the provided arguments are not valid for the tool.
        """
        tool = self.tools.get(tool_name)
        if tool is None:
            raise KeyError(f"Tool '{tool_name}' not found.")

        # Check if all required arguments are provided
        missing_args = set(tool.arguments.keys()) - set(arguments.keys())
        if missing_args:
            raise TypeError(f"Missing arguments for tool '{tool_name}': {', '.join(missing_args)}")

        # Call the tool function
        try:
            result = tool.function(**arguments)
            return result
        except Exception as e:
            raise RuntimeError(f"Error calling tool '{tool_name}': {e}")

    def get_tool_descriptions(self) -> Dict[str, str]:
        """Returns a dictionary of tool names to their descriptions."""
        return {tool.name: tool.description for tool in self.tools.values()}

    def get_tool_description(self, tool_name: str) -> str:
        """Returns the description of the specified tool."""
        tool = self.tools.get(tool_name)
        if tool:
            return tool.description
        else:
            return f"Tool '{tool_name}' not found."

    def retrieve_tools_by_names(self, tool_names: List[str]) -> List[Callable]:
        """
        Retrieves tool functions from the ToolManager based on the provided names.

        Args:
            tool_names: A list of tool names to retrieve.

        Returns:
            A list of tool functions.
        """
        loaded_tools = []
        for tool_name in tool_names:
            tool_function = self.get_tool_function(tool_name)
            if tool_function:
                loaded_tools.append(tool_function)
            else:
                print(f"Tool '{tool_name}' not found.")  # Handle missing tools gracefully
        return loaded_tools

## File: visualisation.py (in: C:\Users\DELL\Desktop\selfawareGemini\SelAwareAI_Gemini\AGI_start_7_working)
import json
from html import escape
import colorsys

def generate_color_scheme(num_colors):
    return [colorsys.hsv_to_rgb(i / num_colors, 0.8, 0.8) for i in range(num_colors)]

def rgb_to_hex(rgb):
    return '#%02x%02x%02x' % tuple(int(x * 255) for x in rgb)

def create_modelium_vis_js(modelium_configs):
    nodes = []
    edges = []
    groups = set()
    chains = set()

    for config_index, modelium_config in enumerate(modelium_configs):
        # Access the 'model_config' list directly
        model_configs = modelium_config['model_config']

        for chain_index, config in enumerate(model_configs):  # chain is now directly the config dict
            chains.add(chain_index)
            node_id = f"{chain_index}_{config['model_name']}"
            groups.add(config['model_type'])
            node = {
                "id": node_id,
                "label": config["model_name"],
                "group": config["model_type"],
                "title": f"<strong>{config['model_name']}</strong><br>"
                         f"<b>Type:</b> {config['model_type']}<br>"
                         f"<b>Access:</b> {config['model_access']}<br>"
                         f"<b>Tool:</b> {config['tool_access']}<br>"
                         f"<b>Check Flags:</b> {config['check_flags']}<br>"
                         f"<b>System Instruction:</b> {escape(config['system_instruction'])}<br>"
                         f"<b>Prompt:</b> {escape(config['prompt'])}",
                "x": chain_index * 300,
                "y": config.get('level', 0) * 200,
                "level": config.get('level', 0),
                "chainIndex": chain_index,
                "modelType": config['model_type'],
                "toolAccess": config['tool_access'],
                "checkFlags": config['check_flags'],
                "chain": chain_index
            }

            if config["tool_access"] != "none":
                node["shape"] = "diamond"
            if config["check_flags"]:
                node["borderWidth"] = 3
                node["borderWidthSelected"] = 5

            nodes.append(node)

            if config["model_access"] != "none":
                parent_id = f"{chain_index}_{config['model_access']}"
                edges.append({
                    "from": parent_id,
                    "to": node_id,
                    "arrows": "to"
                })

            # Add loop edge (example condition, adjust as needed)
            if chain_index % 2 == 0:
                last_node = nodes[-1]["id"]
                first_node = nodes[0]["id"]
                edges.append({
                    "from": last_node,
                    "to": first_node,
                    "arrows": "to",
                    "dashes": True,
                    "label": "Loop"
                })

    vis_data = {
        "nodes": nodes,
        "edges": edges
    }

    color_palette = generate_color_scheme(len(groups))
    group_colors = {group: rgb_to_hex(color) for group, color in zip(groups, color_palette)}

    chain_color_palette = generate_color_scheme(len(chains))
    chain_colors = {chain: rgb_to_hex(color) for chain, color in zip(chains, chain_color_palette)}

    html = f"""
    <!DOCTYPE html>
    <html>
    <head>
        <title>Dynamic Modelium Visualization</title>
        <script type="text/javascript" src="https://unpkg.com/vis-network/standalone/umd/vis-network.min.js"></script>
        <script src="https://cdn.jsdelivr.net/npm/chart.js"></script>
        <style type="text/css">
            body, html {{
                height: 100%;
                margin: 0;
                padding: 0;
                overflow: hidden;
                font-family: Arial, sans-serif;
            }}
            #mynetwork {{
                width: 80%;
                height: 100%;
                float: left;
            }}
            #sidebar {{
                width: 20%;
                height: 100%;
                float: right;
                padding: 10px;
                box-sizing: border-box;
                overflow-y: auto;
                background-color: #f0f0f0;
            }}
            #controls, #configInput {{
                margin-bottom: 20px;
            }}
            #legend {{
                margin-bottom: 20px;
            }}
            .legend-item {{
                margin-bottom: 5px;
            }}
            #nodeInfo {{
                margin-top: 20px;
            }}
            #statsChart {{
                margin-top: 20px;
            }}
            textarea {{
                width: 100%;
                height: 200px;
            }}
        </style>
    </head>
    <body>
    <div id="mynetwork"></div>
    <div id="sidebar">
        <div id="configInput">
            <h3>New Configuration</h3>
            <textarea id="newConfig" placeholder="Paste your new modelium_config here"></textarea>
            <button onclick="updateVisualization()">Update Visualization</button>
        </div>
        <div id="controls">
            <h3>Display Options</h3>
            <label><input type="checkbox" id="showModelType" checked> Show Model Type</label><br>
            <label><input type="checkbox" id="showToolAccess"> Show Tool Access</label><br>
            <label><input type="checkbox" id="showCheckFlags"> Show Check Flags</label><br>
            <label>
                Layout:
                <select id="layoutSelect">
                    <option value="hierarchical">Hierarchical</option>
                    <option value="standard">Standard</option>
                </select>
            </label><br>
            <label>
                Color Scheme:
                <select id="colorSchemeSelect">
                    <option value="modelType">By Model Type</option>
                    <option value="chain">By Chain</option>
                </select>
            </label><br>
            <label><input type="checkbox" id="preventOverlap"> Prevent Overlap</label>
        </div>
        <div id="legend">
            <h3>Legend</h3>
            <div class="legend-item">üîπ Standard Model</div>
            <div class="legend-item">üî∂ Model with Tool Access</div>
            <div class="legend-item">üî∑ Model with Check Flags</div>
            <div class="legend-item">‚û°Ô∏è Model Access Flow</div>
            <div class="legend-item">‚ûø Looping Chain</div>
        </div>
        <div id="nodeInfo">
            <h3>Selected Node Info</h3>
            <p>Click on a node to see details</p>
        </div>
        <div id="statsChart">
            <canvas id="myChart"></canvas>
        </div>
    </div>
    <script type="text/javascript">
        var container = document.getElementById('mynetwork');
        var data = {json.dumps(vis_data)};
        var groupColors = {json.dumps(group_colors)};
        var chainColors = {json.dumps(chain_colors)};

        var options = {{
            layout: {{
                hierarchical: {{
                    enabled: true,
                    direction: 'UD',
                    sortMethod: 'directed',
                    levelSeparation: 150
                }}
            }},
            physics: {{
                enabled: false
            }},
            nodes: {{
                shape: 'box',
                margin: 10,
                widthConstraint: {{
                    minimum: 120,
                    maximum: 250
                }},
                font: {{
                    size: 16
                }}
            }},
            edges: {{
                smooth: {{
                    type: 'cubicBezier',
                    forceDirection: 'vertical',
                    roundness: 0.4
                }}
            }},
            groups: groupColors,
            interaction: {{
                hover: true,
                zoomView: true,
                dragView: true
            }}
        }};

        var network = new vis.Network(container, data, options);

        function updateNodeLabels() {{
            var showModelType = document.getElementById('showModelType').checked;
            var showToolAccess = document.getElementById('showToolAccess').checked;
            var showCheckFlags = document.getElementById('showCheckFlags').checked;

            data.nodes.forEach(function(node) {{
                var label = node.label;
                if (showModelType) label += '\\n' + node.modelType;
                if (showToolAccess) label += '\\n' + (node.toolAccess !== 'none' ? 'üîß' : '');
                if (showCheckFlags) label += '\\n' + (node.checkFlags ? '‚úÖ' : '');
                node.label = label.trim();
            }});

            network.setData({{nodes: data.nodes, edges: data.edges}});
        }}

        function updateColorScheme() {{
            var colorScheme = document.getElementById('colorSchemeSelect').value;
            var colors = colorScheme === 'modelType' ? groupColors : chainColors;
            var colorKey = colorScheme === 'modelType' ? 'group' : 'chain';

            data.nodes.forEach(function(node) {{
                node.color = colors[node[colorKey]];
            }});

            network.setData({{nodes: data.nodes, edges: data.edges}});
        }}

        function toggleOverlapPrevention() {{
            var preventOverlap = document.getElementById('preventOverlap').checked;
            network.setOptions({{
                physics: {{
                    enabled: preventOverlap,
                    repulsion: {{
                        nodeDistance: 150
                    }}
                }}
            }});
        }}

        document.getElementById('showModelType').addEventListener('change', updateNodeLabels);
        document.getElementById('showToolAccess').addEventListener('change', updateNodeLabels);
        document.getElementById('showCheckFlags').addEventListener('change', updateNodeLabels);
        document.getElementById('colorSchemeSelect').addEventListener('change', updateColorScheme);
        document.getElementById('preventOverlap').addEventListener('change', toggleOverlapPrevention);

        document.getElementById('layoutSelect').addEventListener('change', function(event) {{
            var layout = event.target.value;
            if (layout === 'hierarchical') {{
                network.setOptions({{ layout: {{ hierarchical: {{ enabled: true }} }} }});
            }} else {{
                network.setOptions({{ layout: {{ hierarchical: {{ enabled: false }} }} }});
            }}
        }});

        network.on("selectNode", function(params) {{
            var nodeId = params.nodes[0];
            var node = network.body.data.nodes.get(nodeId);
            document.getElementById('nodeInfo').innerHTML = '<h3>' + node.label + '</h3>' + node.title;
            updateChart(node);
        }});

        function updateChart(node) {{
            var ctx = document.getElementById('myChart').getContext('2d');
            new Chart(ctx, {{
                type: 'bar',
                data: {{
                    labels: ['Chain Index', 'Level'],
                    datasets: [{{
                        label: 'Node Position',
                        data: [node.chainIndex, node.level],
                        backgroundColor: [
                            'rgba(75, 192, 192, 0.6)',
                            'rgba(153, 102, 255, 0.6)'
                        ]
                    }}]
                }},
                options: {{
                    scales: {{
                        y: {{
                            beginAtZero: true
                        }}
                    }}
                }}
            }});
        }}

        network.on("afterDrawing", function (ctx) {{
            network.fit({{
                animation: {{
                    duration: 1000,
                    easingFunction: 'easeOutQuint'
                }}
            }});
        }});

        network.on("doubleClick", function(params) {{
            if (params.nodes.length > 0) {{
                network.focus(params.nodes[0], {{
                    scale: 1.5,
                    animation: {{
                        duration: 1000,
                        easingFunction: 'easeOutQuint'
                    }}
                }});
            }}
        }});

        function updateVisualization() {{
            var newConfigText = document.getElementById('newConfig').value;
            try {{
                var newConfig = JSON.parse(newConfigText);
                // Here you would process the new config and update the visualization
                // For demonstration, we'll just log it to the console
                console.log("New configuration received:", newConfig);
                alert("New configuration received. Check the console for details.");
                // In a real implementation, you would update the 'data' variable and redraw the network
            }} catch (error) {{
                alert("Error parsing JSON: " + error.message);
            }}
        }}
    </script>
    </body>
    </html>
    """
    with open("dynamic_modelium_visualization.html", "w", encoding="utf-8") as f:
        f.write(html)
    print("Dynamic Modelium visualization saved to dynamic_modelium_visualization.html")
    return html

## File: what is modelium (in: C:\Users\DELL\Desktop\selfawareGemini\SelAwareAI_Gemini\AGI_start_7_working)
The Challenge:
AI systems are rapidly becoming more sophisticated, involving intricate networks of interconnected models, each with its own unique strengths and capabilities. :brain: Describing these intricate architectures and their functionalities can be daunting, hindering collaboration and innovation in the AI world. :construction:

The Solution:
We need a clear and concise language to describe AI systems ‚Äì one that captures the essence of their model configurations, relationships, and interactions, as well as their dynamic evolution. We introduce the term ‚ÄúEmbedmodelium‚Äù to address this need. :bulb:

What is an ‚ÄúEmbedmodelium‚Äù?
An ‚ÄúEmbedmodelium‚Äù represents a complete AI system, encompassing its model configurations, relationships, and interactions. It‚Äôs a powerful framework for understanding, designing, and discussing AI systems ‚Äì a language that embraces their inherent complexity and adaptability. :rocket:

Introducing ‚ÄúModelium‚Äù:
As we become more familiar with ‚ÄúEmbedmodelium,‚Äù we can shorten it to ‚ÄúModelium‚Äù ‚Äì a concise term that captures the core concept of an interconnected AI system. :zap:

Types of ‚ÄúModeliums‚Äù:
Chain Modelium: Models connected in a sequence, like steps in a process. :arrow_right:

Loop Modelium: Models that execute repeatedly, often with feedback mechanisms. :repeat:

Hierarchical Modelium: Models organized in a management structure. :arrow_up_small:

Parallel Modelium: Models that execute concurrently and independently. :running_woman::man_running:

Ensemble Modelium: A collection of models working together. :handshake:

Variations and Combinations:
Parallel-Chain Modelium: Multiple chains of models running concurrently. :arrow_right::arrow_right::arrow_right:

Loop-Chain Modelium: A chain of models that is repeatedly executed with feedback mechanisms. :arrow_right::repeat:

Parallel-Hierarchical-Looping-Chain Modelium: A complex nested structure combining multiple layers of parallel, hierarchical, looping, and chain configurations. :exploding_head:

The Power of Nested Configurations
We can go even deeper, describing even more complex configurations like ‚ÄúParallel100-Hierarchical2-Chain3 Modelium‚Äù. This would represent a system with:

100 parallel instances. :running_woman::running_woman::running_woman:

Each instance has 2 hierarchical levels. :arrow_up_small::arrow_up_small:

At the lowest level, there is a chain of 3 models. :arrow_right::arrow_right::arrow_right:

The Dynamic Nature of ‚ÄúModeliums‚Äù:
Seed Modeliums: Starting with a diverse set of ‚Äúseed Modeliums,‚Äù we can use rewards and punishments to guide their evolution, selecting the most effective configurations for a specific task. :trophy:

Adaptive Evolution: Modeliums can adapt to new data and changing conditions, constantly refining their structures and interactions to find optimal solutions. :chart_with_upwards_trend:

Why Use ‚ÄúModelium‚Äù?
Clarity and Conciseness: ‚ÄúModelium‚Äù provides a simple yet powerful term to describe complex AI systems. :bulb:

Enhanced Communication: It fosters a shared language for AI researchers, developers, and enthusiasts. :speech_balloon:

Problem-Solving Framework: It provides a conceptual framework for reasoning about AI system design and optimization. :brain:

Flexibility and Adaptability: ‚ÄúModelium‚Äù is a versatile term that can be combined with specific configurations and can evolve with the field of AI. :chart_with_upwards_trend:

Join the ‚ÄúModelium‚Äù Revolution!
Help us shape the future of AI by using ‚ÄúModelium‚Äù and its variations in your discussions, presentations, and research. Let‚Äôs make AI communication clear, concise, and powerful! :boom:

Examples:
‚ÄúWe used a Chain Modelium to analyze the text and extract key information.‚Äù :arrow_right:

‚ÄúThe researchers developed a Loop Modelium for generating creative text.‚Äù :repeat:

‚ÄúA Hierarchical Modelium was implemented to manage the different components of the robot‚Äôs navigation system.‚Äù :arrow_up_small:

‚ÄúThe team designed a Parallel100-Hierarchical2-Chain3 Modelium for analyzing large datasets.‚Äù :running_woman::running_woman::running_woman::arrow_up_small::arrow_up_small::arrow_right::arrow_right::arrow_right:

‚ÄúModelium‚Äù is more than just a term; it‚Äôs a vision for the future of AI.
It‚Äôs a vision of AI systems that are flexible, dynamic, and capable of adapting to new challenges. :brain: It‚Äôs a vision of a future where AI is more accessible, more collaborative, and more powerful than ever before. :handshake:

Let‚Äôs embrace ‚ÄúModelium‚Äù and help shape the future of AI! :star2:



